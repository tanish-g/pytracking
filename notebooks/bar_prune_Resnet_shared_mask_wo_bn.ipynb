{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove the unnecessary filters from a CNN pruned using main.py, then compare the converted net with the previous.\n",
    "While main.py allows to determine which filters can be removed, those remain in the network's architecture. The\n",
    "current script creates a new, lightweight architecture from the result of main.py.\n",
    "\n",
    "NOTE: The arguments passed to this script are parsed in main.py (i.e. a dataset choice must be made).\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import VITALabAI.project.bar.model.resnet as resnet\n",
    "# from VITALabAI.project.bar.model.layers import SparseConvConfig\n",
    "# # from VITALabAI.project.bar.main import ClassificationTraining\n",
    "\n",
    "\n",
    "class IdentityModule(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "def convert_resnet(net, insert_identity_modules=False):\n",
    "    \"\"\"Convert a ResNetCifar module (in place)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        net: the mutated net\n",
    "    \"\"\"\n",
    "    \n",
    "    net.conv1, net.bn1 = convert_conv_bn(net.conv1, net.bn1, torch.ones(3).byte(), (cfg_dict['bn1']>0))\n",
    "    in_gates = torch.ones(net.conv1.out_channels).byte()\n",
    "\n",
    "    clean_res = True\n",
    "    net.layer1, in_gates = convert_layer(net.layer1, in_gates, insert_identity_modules, clean_res, layer_name = 'layer1')\n",
    "    net.layer2, in_gates = convert_layer(net.layer2, in_gates, insert_identity_modules, clean_res, layer_name = 'layer2')\n",
    "    net.layer3, in_gates = convert_layer(net.layer3, in_gates, insert_identity_modules, clean_res, layer_name = 'layer3')\n",
    "    net.layer4, in_gates = convert_layer(net.layer4, in_gates, insert_identity_modules, clean_res, layer_name = 'layer4')\n",
    "\n",
    "\n",
    "    if clean_res:\n",
    "        net.fc = convert_fc_head(net.fc, in_gates)\n",
    "    else:\n",
    "        net.fc = resnet.InwardPrunedLinear(convert_fc_head(net.fc, in_gates), mask2i(in_gates))\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def convert_layer(layer_module, in_gates, insert_identity_modules, clean_res, layer_name =None):\n",
    "    \"\"\"Convert a ResnetCifar layer (in place)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        layer_module: a nn.Sequential\n",
    "        in_gates: mask\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        layer_module: mutated layer_module\n",
    "        in_gates: ajusted mask\n",
    "    \"\"\"\n",
    "\n",
    "    previous_layer_gates = in_gates\n",
    "\n",
    "    new_blocks = []\n",
    "    for block_num, block in enumerate(layer_module):\n",
    "        new_block, in_gates = convert_block(block, in_gates, block_name = layer_name+'.'+str(block_num))\n",
    "        if new_block is None:\n",
    "            if insert_identity_modules:\n",
    "                new_blocks.append(IdentityModule())\n",
    "        else:\n",
    "            new_blocks.append(new_block)\n",
    "\n",
    "    # Remove unused residual features\n",
    "    if clean_res:\n",
    "        print()\n",
    "        cur_layer_gates = in_gates\n",
    "        for block in new_blocks:\n",
    "            if isinstance(block, IdentityModule):\n",
    "                continue\n",
    "            clean_block(block, previous_layer_gates, cur_layer_gates)  # in-place\n",
    "\n",
    "    layer_module = nn.Sequential(*new_blocks)\n",
    "    return layer_module, in_gates\n",
    "\n",
    "\n",
    "def clean_block(mixed_block, previous_layer_alivef, cur_layer_alivef):\n",
    "    \"\"\"Remove unused res features (operates in-place)\"\"\"\n",
    "\n",
    "    def clean_indices(idx, alive_mask=cur_layer_alivef):\n",
    "        mask = i2mask(idx, alive_mask)\n",
    "        mask = mask[mask2i(alive_mask)]\n",
    "        return mask2i(mask)\n",
    "\n",
    "    if mixed_block.f_res is None:\n",
    "        mixed_block.in_idx = clean_indices(mixed_block.in_idx)\n",
    "    else:\n",
    "        mixed_block.in_idx = clean_indices(mixed_block.in_idx, alive_mask=previous_layer_alivef)\n",
    "        mixed_block.res_size = cur_layer_alivef.sum().item()\n",
    "        print('DOWNS ----- Res size: ', mixed_block.res_size)\n",
    "        mixed_block.res_idx = clean_indices(mixed_block.res_idx)\n",
    "        print('Res:  ', len(mixed_block.res_idx))\n",
    "    mixed_block.delta_idx = clean_indices(mixed_block.delta_idx)\n",
    "\n",
    "    print('In:   ', len(mixed_block.in_idx))\n",
    "    print('Delta:', len(mixed_block.delta_idx))\n",
    "\n",
    "\n",
    "def convert_block(block_module, in_gates, block_name = None):\n",
    "    \"\"\"Convert a Basic Resnet block (in place)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        block_module: a BasicBlock\n",
    "        in_gates: received mask\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        block_module: mutated block\n",
    "        in_gates: out_gates of this block (in_gates for next block)\n",
    "    \"\"\"\n",
    "\n",
    "#     assert not hasattr(block_module, 'conv3')  # must be basic block\n",
    "\n",
    "    b1_gates = (cfg_dict[f'{block_name}.bn1']>0)   # get_gates(block_module.bn1)\n",
    "    b2_gates = (cfg_dict[f'{block_name}.bn2']>0)\n",
    "    b3_gates = (cfg_dict[f'{block_name}.bn3']>0)\n",
    "   \n",
    "\n",
    "    delta_branch_is_pruned = b1_gates.sum().item() == 0 or b2_gates.sum().item() == 0 or b3_gates.sum().item() == 0\n",
    "    \n",
    "    # Delta branch\n",
    "    if not delta_branch_is_pruned:\n",
    "        block_module.conv1, block_module.bn1 = convert_conv_bn(block_module.conv1, block_module.bn1, in_gates, b1_gates)\n",
    "        block_module.conv2, block_module.bn2 = convert_conv_bn(block_module.conv2, block_module.bn2, b1_gates, b2_gates)\n",
    "        block_module.conv3, block_module.bn3 = convert_conv_bn(block_module.conv3, block_module.bn3, b2_gates, b3_gates)\n",
    "\n",
    "\n",
    "    if block_module.downsample is not None:\n",
    "        ds_gates = (cfg_dict[f'{block_name}.downsample.1']>0) # get_gates(block_module.downsample[1])\n",
    "        ds_conv, ds_bn = convert_conv_bn(block_module.downsample[0], block_module.downsample[1], in_gates, ds_gates)\n",
    "        ds_module = nn.Sequential(ds_conv, ds_bn)\n",
    "\n",
    "        if delta_branch_is_pruned:\n",
    "            mixed_block = MixedBlock(f_delta=None, delta_idx=None,\n",
    "                                            f_res=ds_module,\n",
    "                                            in_idx=mask2i(in_gates),\n",
    "                                            res_idx=mask2i(ds_gates),\n",
    "                                            res_size=len(b3_gates))\n",
    "        else:\n",
    "            block_module.downsample = ds_module\n",
    "            mixed_block = MixedBlock.from_bottleneck(block_module,\n",
    "                                                       delta_idx=mask2i(b3_gates),\n",
    "                                                       in_idx=mask2i(in_gates),\n",
    "                                                       res_idx=mask2i(ds_gates),\n",
    "                                                       res_size=len(b3_gates))\n",
    "        in_gates = elementwise_or(ds_gates, b3_gates)\n",
    "    else:\n",
    "        if delta_branch_is_pruned:\n",
    "            mixed_block = None\n",
    "        else:\n",
    "            mixed_block = MixedBlock.from_bottleneck(block_module,\n",
    "                                                       delta_idx=mask2i(b3_gates),\n",
    "                                                       in_idx=mask2i(in_gates))\n",
    "        in_gates = elementwise_or(in_gates, b3_gates)\n",
    "\n",
    "    return mixed_block, in_gates\n",
    "\n",
    "\n",
    "def convert_conv_bn(conv_module, bn_module, in_gates, out_gates):\n",
    "    in_indices = mask2i(in_gates)  # indices of kept features\n",
    "    out_indices = mask2i(out_gates)\n",
    "\n",
    "    # Keep the good ones\n",
    "    new_conv_w = conv_module.weight.data[out_indices][:, in_indices]\n",
    "\n",
    "    new_conv = make_conv(new_conv_w, from_module=conv_module)\n",
    "    new_bn = convert_bn(bn_module, out_indices)\n",
    "\n",
    "    new_conv.out_idx = out_indices\n",
    "    \n",
    "    return new_conv, new_bn\n",
    "\n",
    "\n",
    "def convert_fc_head(fc_module, in_gates):\n",
    "    \"\"\"Convert a the final FC module of the net\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        fc_module: a nn.Linear with weight tensor of size (out_f, in_f)\n",
    "        in_gates: binary vector or list of size in_f\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        fc_module: mutated module\n",
    "    \"\"\"\n",
    "\n",
    "    in_indices = mask2i(in_gates)\n",
    "    new_weight_tensor = fc_module.weight.data[:, in_indices]\n",
    "    return make_fc(new_weight_tensor, from_module=fc_module)\n",
    "\n",
    "\n",
    "def convert_bn(bn_module, out_indices):\n",
    "#     z = bn_module.get_gates(stochastic=False)\n",
    "    new_weight = bn_module.weight.data[out_indices] # * z[out_indices]\n",
    "    new_bias = bn_module.bias.data[out_indices] # * z[out_indices]\n",
    "\n",
    "    new_bn_module = nn.BatchNorm2d(len(new_weight))\n",
    "    new_bn_module.weight.data.copy_(new_weight)\n",
    "    new_bn_module.bias.data.copy_(new_bias)\n",
    "    new_bn_module.running_mean.copy_(bn_module.running_mean[out_indices])\n",
    "    new_bn_module.running_var.copy_(bn_module.running_var[out_indices])\n",
    "\n",
    "    new_bn_module.out_idx = out_indices\n",
    "\n",
    "    return new_bn_module\n",
    "\n",
    "\n",
    "def make_bn(bn_module, kept_indices):\n",
    "    new_bn_module = nn.BatchNorm2d(len(kept_indices))\n",
    "    new_bn_module.weight.data.copy_(bn_module.weight.data[kept_indices])\n",
    "    new_bn_module.bias.data.copy_(bn_module.bias.data[kept_indices])\n",
    "    new_bn_module.running_mean.copy_(bn_module.running_mean[kept_indices])\n",
    "    new_bn_module.running_var.copy_(bn_module.running_var[kept_indices])\n",
    "\n",
    "    if hasattr(bn_module, 'out_idx'):\n",
    "        new_bn_module.out_idx = bn_module.out_idx[kept_indices]\n",
    "    else:\n",
    "        new_bn_module.out_idx = kept_indices\n",
    "\n",
    "    return new_bn_module\n",
    "\n",
    "\n",
    "def make_conv(weight_tensor, from_module):\n",
    "    # NOTE: No bias\n",
    "\n",
    "    # New weight size\n",
    "    in_channels = weight_tensor.size(1)\n",
    "    out_channels = weight_tensor.size(0)\n",
    "\n",
    "    # Other params\n",
    "    kernel_size = from_module.kernel_size\n",
    "    stride = from_module.stride\n",
    "    padding = from_module.padding\n",
    "\n",
    "    conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    conv.weight.data.copy_(weight_tensor)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def make_fc(weight_tensor, from_module):\n",
    "    in_features = weight_tensor.size(1)\n",
    "    out_features = weight_tensor.size(0)\n",
    "    fc = nn.Linear(in_features, out_features)\n",
    "    fc.weight.data.copy_(weight_tensor)\n",
    "    fc.bias.data.copy_(from_module.bias.data)\n",
    "    return fc\n",
    "\n",
    "def elementwise_or(a, b):\n",
    "    return (a + b) > 0\n",
    "\n",
    "\n",
    "def mask2i(mask):\n",
    "#     assert mask.dtype == torch.uint8\n",
    "    return mask.nonzero().view(-1)  # Note: do not use .squeeze() because single item becomes a scalar instead of 1-vec\n",
    "\n",
    "\n",
    "def i2mask(i, from_tensor):\n",
    "    x = torch.zeros_like(from_tensor)\n",
    "    x[i] = 1\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedBlock(nn.Module):\n",
    "    def __init__(self, f_delta, delta_idx, in_idx, f_res=None, res_idx=None, res_size=None):\n",
    "        super(MixedBlock, self).__init__()\n",
    "        self.f_delta = f_delta\n",
    "        self.delta_idx = delta_idx\n",
    "        self.in_idx = in_idx\n",
    "        self.f_res = f_res\n",
    "        self.res_idx = res_idx\n",
    "        self.res_size = res_size\n",
    "        self.activ = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.res_scatter_idx = None\n",
    "        self.delta_scatter_idx = None\n",
    "\n",
    "        if f_delta is None:\n",
    "            self.forward = self.forward_without_delta\n",
    "        else:\n",
    "            self.forward = self.forward_with_delta\n",
    "\n",
    "    def scatter_features(self, idx, src, final_size):\n",
    "        if self.res_scatter_idx is None or self.res_scatter_idx.size(0) != src.size(0):\n",
    "            scatter_idx = idx.new_empty(*src.size())\n",
    "            scatter_idx.copy_(idx[None, :, None, None])\n",
    "            self.res_scatter_idx = scatter_idx\n",
    "\n",
    "        x = torch.zeros(src.size(0), final_size, src.size(2), src.size(3)).to(src)\n",
    "        x.scatter_(dim=1, index=self.res_scatter_idx, src=src)\n",
    "        return x\n",
    "\n",
    "    def scatter_add_features(self, dst, idx, src):\n",
    "        if self.delta_scatter_idx is None or self.delta_scatter_idx.size(0) != src.size(0):\n",
    "            scatter_idx = idx.new_empty(*src.size())\n",
    "            scatter_idx.copy_(idx[None, :, None, None])\n",
    "            self.delta_scatter_idx = scatter_idx\n",
    "\n",
    "        dst.scatter_add_(dim=1, index=self.delta_scatter_idx, src=src)\n",
    "\n",
    "    def forward_with_delta(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "\n",
    "        if self.f_res is None:\n",
    "            x_alive = x.index_select(dim=1, index=self.in_idx)\n",
    "            delta = self.f_delta.forward(x_alive)  # 3x3 conv\n",
    "        else:\n",
    "            delta = self.f_delta.forward(x)  # 3x3 conv\n",
    "\n",
    "            res = self.f_res.forward(x)  # 1x1 conv\n",
    "            x = self.scatter_features(self.res_idx, res, self.res_size)\n",
    "\n",
    "        self.scatter_add_features(x, self.delta_idx, delta)\n",
    "        \n",
    "        return self.activ(x)\n",
    "\n",
    "    def forward_without_delta(self, x):\n",
    "        res = self.f_res.forward(x)  # 1x1 conv\n",
    "        x = self.scatter_features(self.res_idx, res, self.res_size)\n",
    "\n",
    "        return self.activ(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_basic(block, delta_idx, in_idx, res_idx=None, res_size=None):\n",
    "        f_delta = nn.Sequential(\n",
    "            block.conv1,\n",
    "            block.bn1,\n",
    "            block.activ,  # nn.ReLU(inplace=True)\n",
    "            block.conv2,\n",
    "            block.bn2\n",
    "        )\n",
    "        return MixedBlock(f_delta, delta_idx, in_idx, block.downsample, res_idx, res_size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_bottleneck(block, delta_idx, in_idx, res_idx=None, res_size=None):\n",
    "        f_delta = nn.Sequential(\n",
    "            block.conv1,\n",
    "            block.bn1,\n",
    "            block.relu,  # nn.ReLU(inplace=True)\n",
    "            block.conv2,\n",
    "            block.bn2,\n",
    "            block.relu,\n",
    "            block.conv3,\n",
    "            block.bn3\n",
    "        )\n",
    "        return MixedBlock(f_delta, delta_idx, in_idx, block.downsample, res_idx, res_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /workspace/pytracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet50_single_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/pytracking\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/pytracking\n",
    "from ltr.models.backbone.resnet import resnet50\n",
    "from ltr.models.backbone.resnet_child import resnet50_child\n",
    "from ltr.models.backbone.resnet_shared_mask_wo_bn import resnet50_mask_wo_bn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50_mask_wo_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.admin.loading import torch_load_legacy\n",
    "ckpt = torch_load_legacy('/workspace/tracking_datasets/saved_ckpts/ltr/dimp/sparse/dimp50_mask_wo_bn/DiMPnet_ep0049.pth.tar')['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['initializer.filter_conv.weight', 'initializer.filter_conv.bias', 'optimizer.log_step_length', 'optimizer.filter_reg', 'optimizer.label_map_predictor.weight', 'optimizer.target_mask_predictor.0.weight', 'optimizer.spatial_weight_predictor.weight', '_extractor.0.weight', '_1r.0.weight', '_1r.0.bias', '_1r.1.weight', '_1r.1.bias', '_1r.1.running_mean', '_1r.1.running_var', '_1r.1.num_batches_tracked', '_1t.0.weight', '_1t.0.bias', '_1t.1.weight', '_1t.1.bias', '_1t.1.running_mean', '_1t.1.running_var', '_1t.1.num_batches_tracked', '_2t.0.weight', '_2t.0.bias', '_2t.1.weight', '_2t.1.bias', '_2t.1.running_mean', '_2t.1.running_var', '_2t.1.num_batches_tracked', 'r.0.weight', 'r.0.bias', 'r.1.weight', 'r.1.bias', 'r.1.running_mean', 'r.1.running_var', 'r.1.num_batches_tracked', '3r.0.weight', '3r.0.bias', '3r.1.weight', '3r.1.bias', '3r.1.running_mean', '3r.1.running_var', '3r.1.num_batches_tracked', '4r.0.weight', '4r.0.bias', '4r.1.weight', '4r.1.bias', '4r.1.running_mean', '4r.1.running_var', '4r.1.num_batches_tracked', 't.linear.weight', 't.linear.bias', 't.bn.weight', 't.bn.bias', 't.bn.running_mean', 't.bn.running_var', 't.bn.num_batches_tracked', 'redictor.weight', 'redictor.bias'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "new_state = OrderedDict()\n",
    "\n",
    "for key, value in ckpt.items():\n",
    "    key = key[18:] # remove `module.`\n",
    "    new_state[key] = value\n",
    "model.load_state_dict(new_state, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision.models.resnet import model_urls\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    \"\"\"Base class for backbone networks. Handles freezing layers etc.\n",
    "    args:\n",
    "        frozen_layers  -  Name of layers to freeze. Either list of strings, 'none' or 'all'. Default: 'none'.\n",
    "    \"\"\"\n",
    "    def __init__(self, frozen_layers=()):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(frozen_layers, str):\n",
    "            if frozen_layers.lower() == 'none':\n",
    "                frozen_layers = ()\n",
    "            elif frozen_layers.lower() != 'all':\n",
    "                raise ValueError('Unknown option for frozen layers: \\\"{}\\\". Should be \\\"all\\\", \\\"none\\\" or list of layer names.'.format(frozen_layers))\n",
    "\n",
    "        self.frozen_layers = frozen_layers\n",
    "        self._is_frozen_nograd = False\n",
    "\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode == True:\n",
    "            self._set_frozen_to_eval()\n",
    "        if not self._is_frozen_nograd:\n",
    "            self._set_frozen_to_nograd()\n",
    "            self._is_frozen_nograd = True\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _set_frozen_to_eval(self):\n",
    "        if isinstance(self.frozen_layers, str) and self.frozen_layers.lower() == 'all':\n",
    "            self.eval()\n",
    "        else:\n",
    "            for layer in self.frozen_layers:\n",
    "                getattr(self, layer).eval()\n",
    "\n",
    "\n",
    "    def _set_frozen_to_nograd(self):\n",
    "        if isinstance(self.frozen_layers, str) and self.frozen_layers.lower() == 'all':\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad_(False)\n",
    "        else:\n",
    "            for layer in self.frozen_layers:\n",
    "                for p in getattr(self, layer).parameters():\n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "class scaler(nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "        super(scaler, self).__init__()\n",
    "        self.weight = nn.parameter.Parameter(torch.empty(num_features)).reshape(1,num_features,1,1).cuda()\n",
    "        self.weight.retain_grad()\n",
    "    def forward(self,x):\n",
    "        out = self.weight * x\n",
    "        return out\n",
    "    \n",
    "\n",
    "class channel_selection(nn.Module):\n",
    "    \"\"\"\n",
    "    Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer.\n",
    "    The output shape of this layer is determined by the number of 1 in `self.indexes`.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels):\n",
    "        \"\"\"\n",
    "        Initialize the `indexes` with all one vector with the length same as the number of channels.\n",
    "        During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0.\n",
    "        \"\"\"\n",
    "        super(channel_selection, self).__init__()\n",
    "        self.indexes = nn.Parameter(torch.ones(num_channels))\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Parameter\n",
    "        ---------\n",
    "        input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer.\n",
    "        \"\"\"\n",
    "        selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy()))\n",
    "        if selected_index.size == 1:\n",
    "            selected_index = np.resize(selected_index, (1,)) \n",
    "        output = input_tensor[:, :, :, :]\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, use_bn=True):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)\n",
    "\n",
    "        if use_bn:\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, dilation=dilation)\n",
    "\n",
    "        if use_bn:\n",
    "            self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        if self.use_bn:\n",
    "            out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        if self.use_bn:\n",
    "            out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes,cfg,stride=1, downsample=None, dilation=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(cfg[0],cfg[1], kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(cfg[1])\n",
    "        self.conv2 = nn.Conv2d(cfg[1],cfg[2], kernel_size=3, stride=stride,\n",
    "                               padding=dilation, bias=False, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(cfg[2])\n",
    "        self.conv3 = nn.Conv2d(cfg[2],cfg[3], kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(cfg[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "#         self.mask = scaler(cfg[3])\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "#         out = self.mask(out)\n",
    "#         residual = self.mask(residual)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(Backbone):\n",
    "    \"\"\" ResNet network module. Allows extracting specific feature blocks.\"\"\"\n",
    "    def __init__(self, block, layers, output_layers,cfg = None , num_classes=1000, inplanes=64, dilation_factor=1, frozen_layers=()):\n",
    "        self.inplanes = inplanes\n",
    "        super(ResNet, self).__init__(frozen_layers=frozen_layers)\n",
    "        self.output_layers = output_layers\n",
    "        self.conv1 = nn.Conv2d(3, inplanes , kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        stride = [1 + (dilation_factor < l) for l in (8, 4, 2)]\n",
    "        self.layer1 = self._make_layer(block, inplanes, layers[0], dilation=max(dilation_factor//8, 1), cfg = cfg[0:(3*layers[0])+1])\n",
    "        self.layer2 = self._make_layer(block, inplanes*2, layers[1], stride=stride[0], dilation=max(dilation_factor//4, 1), cfg = cfg[3*layers[0]:3*layers[1]+3*layers[0]+1])\n",
    "        self.layer3 = self._make_layer(block, inplanes*4, layers[2], stride=stride[1], dilation=max(dilation_factor//2, 1), cfg = cfg[3*layers[1]+3*layers[0]:3*layers[1]+3*layers[0]+3*layers[2]+1])\n",
    "        self.layer4 = self._make_layer(block, inplanes*8, layers[3], stride=stride[2], dilation=dilation_factor, cfg = cfg[3*layers[1]+3*layers[0]+3*layers[2]:3*layers[1]+3*layers[0]+3*layers[2]+3*layers[3]+1])\n",
    "\n",
    "        out_feature_strides = {'conv1': 4, 'layer1': 4, 'layer2': 4*stride[0], 'layer3': 4*stride[0]*stride[1],\n",
    "                               'layer4': 4*stride[0]*stride[1]*stride[2]}\n",
    "\n",
    "        # TODO better way?\n",
    "        if isinstance(self.layer1[0], BasicBlock):\n",
    "            out_feature_channels = {'conv1': inplanes, 'layer1': inplanes, 'layer2': inplanes*2, 'layer3': inplanes*4,\n",
    "                               'layer4': inplanes*8}\n",
    "        elif isinstance(self.layer1[0], Bottleneck):\n",
    "            base_num_channels = 4 * inplanes\n",
    "            out_feature_channels = {'conv1': inplanes, 'layer1': base_num_channels, 'layer2': base_num_channels * 2,\n",
    "                                    'layer3': base_num_channels * 4, 'layer4': base_num_channels * 8}\n",
    "        else:\n",
    "            raise Exception('block not supported')\n",
    "\n",
    "        self._out_feature_strides = out_feature_strides\n",
    "        self._out_feature_channels = out_feature_channels\n",
    "\n",
    "        # self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(cfg[-1], num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def out_feature_strides(self, layer=None):\n",
    "        if layer is None:\n",
    "            return self._out_feature_strides\n",
    "        else:\n",
    "            return self._out_feature_strides[layer]\n",
    "\n",
    "    def out_feature_channels(self, layer=None):\n",
    "        if layer is None:\n",
    "            return self._out_feature_channels\n",
    "        else:\n",
    "            return self._out_feature_channels[layer]\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, cfg=None):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(cfg[0],cfg[3],\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(cfg[3]),\n",
    "#                 channel_selection(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, cfg[0:4], stride, downsample, dilation=dilation))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "#             print(3*i,(3*(i+1))+1)\n",
    "#             print(cfg[3*i:(3*(i+1))+1],(3*(i+1))+1 - (3*i))\n",
    "#             print(cfg[9])\n",
    "            layers.append(block(self.inplanes, planes, cfg[3*i:(3*(i+1))+1]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _add_output_and_check(self, name, x, outputs, output_layers):\n",
    "        if name in output_layers:\n",
    "            outputs[name] = x\n",
    "        return len(output_layers) == len(outputs)\n",
    "\n",
    "    def forward(self, x, output_layers=None):\n",
    "        \"\"\" Forward pass with input x. The output_layers specify the feature blocks which must be returned \"\"\"\n",
    "        outputs = OrderedDict()\n",
    "\n",
    "        if output_layers is None:\n",
    "            output_layers = self.output_layers\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x) ####### select daal dena\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self._add_output_and_check('conv1', x, outputs, output_layers):\n",
    "            return outputs\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        if self._add_output_and_check('layer1', x, outputs, output_layers):\n",
    "            return outputs\n",
    "\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        if self._add_output_and_check('layer2', x, outputs, output_layers):\n",
    "            return outputs\n",
    "\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        if self._add_output_and_check('layer3', x, outputs, output_layers):\n",
    "            return outputs\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self._add_output_and_check('layer4', x, outputs, output_layers):\n",
    "            return outputs\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if self._add_output_and_check('fc', x, outputs, output_layers):\n",
    "            return outputs\n",
    "\n",
    "        if len(output_layers) == 1 and output_layers[0] == 'default':\n",
    "            return x\n",
    "\n",
    "        raise ValueError('output_layer is wrong.')\n",
    "\n",
    "\n",
    "def resnet_baby(output_layers=None, pretrained=False, inplanes=16, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "\n",
    "    if output_layers is None:\n",
    "        output_layers = ['default']\n",
    "    else:\n",
    "        for l in output_layers:\n",
    "            if l not in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'fc']:\n",
    "                raise ValueError('Unknown layer: {}'.format(l))\n",
    "\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], output_layers, inplanes=inplanes, **kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        raise NotImplementedError\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(output_layers=None, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "\n",
    "    if output_layers is None:\n",
    "        output_layers = ['default']\n",
    "    else:\n",
    "        for l in output_layers:\n",
    "            if l not in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'fc']:\n",
    "                raise ValueError('Unknown layer: {}'.format(l))\n",
    "\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], output_layers, **kwargs)\n",
    "\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50_child(output_layers=None, pretrained=False,cfg = None,**kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "\n",
    "    if output_layers is None:\n",
    "        output_layers = ['default']\n",
    "    else:\n",
    "        for l in output_layers:\n",
    "            if l not in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'fc']:\n",
    "                raise ValueError('Unknown layer: {}'.format(l))\n",
    "#     ckpt = torch.load('/workspace/tracking_datasets/pruned_ckpts/dimp50_correct/dimp50_correct.pth.tar')\n",
    "#     cfg = ckpt['cfg']\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3],output_layers,cfg=cfg,**kwargs)\n",
    "#     if pretrained:\n",
    "# #         model.load_state_dict(model_zoo.load_url(model_urls['resnet50'], progress = False), strict = False )\n",
    "#           model.load_state_dict(ckpt['state_dict'])\n",
    "#           print('pruned checkpoint loaded')\n",
    "    return model\n",
    "\n",
    "def resnet101_child(output_layers=None, pretrained=False,cfg = None,**kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "\n",
    "    if output_layers is None:\n",
    "        output_layers = ['default']\n",
    "    else:\n",
    "        for l in output_layers:\n",
    "            if l not in ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'fc']:\n",
    "                raise ValueError('Unknown layer: {}'.format(l))\n",
    "#     ckpt = torch.load('/workspace/tracking_datasets/pruned_ckpts/dimp50_correct/dimp50_correct.pth.tar')\n",
    "#     cfg = ckpt['cfg']\n",
    "    model = ResNet(Bottleneck,[3, 4, 23, 3], output_layers,cfg=cfg,**kwargs)\n",
    "#     if pretrained:\n",
    "# #         model.load_state_dict(model_zoo.load_url(model_urls['resnet50'], progress = False), strict = False )\n",
    "#           model.load_state_dict(ckpt['state_dict'])\n",
    "#           print('pruned checkpoint loaded')\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.sum((model.layer1[0].bn3.weight).cpu().numpy()*1)\n",
    "# (model.layer1[0].mask.weight.detach().cpu().numpy()<0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (idx, m) in model.named_modules():\n",
    "#     if isinstance(m, nn.BatchNorm2d):\n",
    "#         print(m.weight.data.shape[0])\n",
    "#     if idx.split('.')[-1] == 'mask':\n",
    "#         print(m.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total1 = 0\n",
    "total2 = 0\n",
    "total3 = 0\n",
    "for idx, m in model.named_modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if idx.split('.')[0] == 'layer1':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                total1 += m.weight.data.shape[0]\n",
    "        if idx.split('.')[0] == 'layer2':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                total2 += m.weight.data.shape[0]\n",
    "        if idx.split('.')[0] == 'layer3':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                total3 += m.weight.data.shape[0]\n",
    "\n",
    "bn1 = torch.zeros(total1)\n",
    "bn2 = torch.zeros(total2)\n",
    "bn3 = torch.zeros(total3)\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "index3 = 0\n",
    "for (idx, m) in model.named_modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if idx.split('.')[0] == 'layer1':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                size = m.weight.data.shape[0]\n",
    "#                 print(size)\n",
    "                bn1[index1:(index1+size)] = m.weight.data.abs().clone()\n",
    "                index1 += size\n",
    "        if idx.split('.')[0] == 'layer2':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                size = m.weight.data.shape[0]\n",
    "                bn2[index2:(index2+size)] = m.weight.data.abs().clone()\n",
    "                index2 += size\n",
    "        if idx.split('.')[0] == 'layer3':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                size = m.weight.data.shape[0]\n",
    "                bn3[index3:(index3+size)] = m.weight.data.abs().clone()\n",
    "                index3 += size\n",
    "                \n",
    "#     if idx.split('.')[-1] == 'mask':\n",
    "#         if idx.split('.')[0] == 'layer1':\n",
    "#             size = m.weight.data.shape[1]\n",
    "#             bn1[index1:(index1+size)] = m.weight.data.abs().clone()[0,:,0,0]\n",
    "#             index1 += size\n",
    "#         if idx.split('.')[0] == 'layer2':\n",
    "#             size = m.weight.data.shape[1]\n",
    "#             bn2[index2:(index2+size)] = m.weight.data.abs().clone()[0,:,0,0]\n",
    "#             index2 += size\n",
    "#         if idx.split('.')[0] == 'layer3':\n",
    "#             size = m.weight.data.shape[1]\n",
    "#             bn3[index3:(index3+size)] = m.weight.data.abs().clone()[0,:,0,0]\n",
    "#             index3 += size\n",
    "\n",
    "            \n",
    "y1, i = torch.sort(bn1)\n",
    "thre_index1 = int(total1 * 0.5)\n",
    "thre1 = y1[thre_index1]\n",
    "\n",
    "y2, i = torch.sort(bn2)\n",
    "thre_index2 = int(total2 * 0.5)\n",
    "thre2 = y2[thre_index2]\n",
    "\n",
    "y3, i = torch.sort(bn3)\n",
    "thre_index3 = int(total3 * 0.5)\n",
    "thre3 = y3[thre_index3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(64.)\n",
      "layer index: 2 \t total channel: 64 \t remaining channel: 64\n",
      "tensor(27.)\n",
      "layer index: 8 \t total channel: 64 \t remaining channel: 27\n",
      "tensor(27.)\n",
      "layer index: 10 \t total channel: 64 \t remaining channel: 27\n",
      "tensor(0.4979, device='cuda:0')\n",
      "layer index: 14 \t total channel: 256 \t remaining channel: 127\n",
      "tensor(24.)\n",
      "layer index: 19 \t total channel: 64 \t remaining channel: 24\n",
      "tensor(29.)\n",
      "layer index: 21 \t total channel: 64 \t remaining channel: 29\n",
      "tensor(0.5002, device='cuda:0')\n",
      "layer index: 25 \t total channel: 256 \t remaining channel: 127\n",
      "tensor(28.)\n",
      "layer index: 28 \t total channel: 64 \t remaining channel: 28\n",
      "tensor(56.)\n",
      "layer index: 30 \t total channel: 64 \t remaining channel: 56\n",
      "tensor(0.4976, device='cuda:0')\n",
      "layer index: 34 \t total channel: 256 \t remaining channel: 127\n",
      "tensor(102.)\n",
      "layer index: 38 \t total channel: 128 \t remaining channel: 102\n",
      "tensor(105.)\n",
      "layer index: 40 \t total channel: 128 \t remaining channel: 105\n",
      "tensor(0.5003, device='cuda:0')\n",
      "layer index: 44 \t total channel: 512 \t remaining channel: 255\n",
      "tensor(5.)\n",
      "layer index: 49 \t total channel: 128 \t remaining channel: 5\n",
      "tensor(40.)\n",
      "layer index: 51 \t total channel: 128 \t remaining channel: 40\n",
      "tensor(0.5019, device='cuda:0')\n",
      "layer index: 55 \t total channel: 512 \t remaining channel: 255\n",
      "tensor(54.)\n",
      "layer index: 58 \t total channel: 128 \t remaining channel: 54\n",
      "tensor(69.)\n",
      "layer index: 60 \t total channel: 128 \t remaining channel: 69\n",
      "tensor(0.5007, device='cuda:0')\n",
      "layer index: 64 \t total channel: 512 \t remaining channel: 255\n",
      "tensor(43.)\n",
      "layer index: 67 \t total channel: 128 \t remaining channel: 43\n",
      "tensor(93.)\n",
      "layer index: 69 \t total channel: 128 \t remaining channel: 93\n",
      "tensor(0.4942, device='cuda:0')\n",
      "layer index: 73 \t total channel: 512 \t remaining channel: 255\n",
      "tensor(247.)\n",
      "layer index: 77 \t total channel: 256 \t remaining channel: 247\n",
      "tensor(176.)\n",
      "layer index: 79 \t total channel: 256 \t remaining channel: 176\n",
      "tensor(0.4954, device='cuda:0')\n",
      "layer index: 83 \t total channel: 1024 \t remaining channel: 511\n",
      "tensor(50.)\n",
      "layer index: 88 \t total channel: 256 \t remaining channel: 50\n",
      "tensor(156.)\n",
      "layer index: 90 \t total channel: 256 \t remaining channel: 156\n",
      "tensor(0.4979, device='cuda:0')\n",
      "layer index: 94 \t total channel: 1024 \t remaining channel: 511\n",
      "tensor(47.)\n",
      "layer index: 97 \t total channel: 256 \t remaining channel: 47\n",
      "tensor(171.)\n",
      "layer index: 99 \t total channel: 256 \t remaining channel: 171\n",
      "tensor(0.4991, device='cuda:0')\n",
      "layer index: 103 \t total channel: 1024 \t remaining channel: 511\n",
      "tensor(81.)\n",
      "layer index: 106 \t total channel: 256 \t remaining channel: 81\n",
      "tensor(89.)\n",
      "layer index: 108 \t total channel: 256 \t remaining channel: 89\n",
      "tensor(0.4981, device='cuda:0')\n",
      "layer index: 112 \t total channel: 1024 \t remaining channel: 511\n",
      "tensor(84.)\n",
      "layer index: 115 \t total channel: 256 \t remaining channel: 84\n",
      "tensor(122.)\n",
      "layer index: 117 \t total channel: 256 \t remaining channel: 122\n",
      "tensor(0.4970, device='cuda:0')\n",
      "layer index: 121 \t total channel: 1024 \t remaining channel: 511\n",
      "tensor(146.)\n",
      "layer index: 124 \t total channel: 256 \t remaining channel: 146\n",
      "tensor(166.)\n",
      "layer index: 126 \t total channel: 256 \t remaining channel: 166\n",
      "tensor(0.4917, device='cuda:0')\n",
      "layer index: 130 \t total channel: 1024 \t remaining channel: 511\n",
      "tensor(512.)\n",
      "layer index: 134 \t total channel: 512 \t remaining channel: 512\n",
      "tensor(512.)\n",
      "layer index: 136 \t total channel: 512 \t remaining channel: 512\n",
      "tensor(2048.)\n",
      "layer index: 138 \t total channel: 2048 \t remaining channel: 2048\n",
      "tensor(512.)\n",
      "layer index: 145 \t total channel: 512 \t remaining channel: 512\n",
      "tensor(512.)\n",
      "layer index: 147 \t total channel: 512 \t remaining channel: 512\n",
      "tensor(2048.)\n",
      "layer index: 149 \t total channel: 2048 \t remaining channel: 2048\n",
      "tensor(512.)\n",
      "layer index: 154 \t total channel: 512 \t remaining channel: 512\n",
      "tensor(512.)\n",
      "layer index: 156 \t total channel: 512 \t remaining channel: 512\n",
      "tensor(2048.)\n",
      "layer index: 158 \t total channel: 2048 \t remaining channel: 2048\n"
     ]
    }
   ],
   "source": [
    "newmodel = resnet50()\n",
    "\n",
    "\n",
    "pruned = 0\n",
    "cfg = []\n",
    "cfg_mask = []\n",
    "modules = list(model.named_modules())\n",
    "\n",
    "old_modules = list(model.modules())\n",
    "new_modules = list(newmodel.modules())\n",
    "cfg_dict = {}\n",
    "\n",
    "for k, (idx, m) in enumerate(modules):\n",
    "    \n",
    "    if isinstance(m, nn.BatchNorm2d) :\n",
    "        if idx.split('.')[0] == 'layer1':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                weight_copy = m.weight.data.abs().clone()\n",
    "                mask = weight_copy.gt(thre1).float()\n",
    "    #             m.wmul_eight.data = weight_copy*mask\n",
    "\n",
    "                pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "                m.weight.data.mul_(mask)\n",
    "                m.bias.data.mul_(mask)\n",
    "    #             if not isinstance(modules[k-2],nn.Sequential):\n",
    "    #             print(idx.split('.')[1])\n",
    "                if not idx.split('.')[2]=='downsample':\n",
    "                    cfg.append(int(torch.sum(mask)))\n",
    "                    print(torch.sum(mask))\n",
    "                else:\n",
    "                    print('yes')\n",
    "                    print(torch.sum(mask),'**')\n",
    "        #             print(modules[k-2])\n",
    "                cfg_mask.append(mask.clone())\n",
    "                cfg_dict[idx] = mask\n",
    "                print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif idx.split('.')[0] == 'layer2':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                weight_copy = m.weight.data.abs().clone()\n",
    "                mask = weight_copy.gt(thre2).float()\n",
    "                pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "                m.weight.data.mul_(mask)\n",
    "                m.bias.data.mul_(mask)\n",
    "    #             if not isinstance(modules[k-2],nn.Sequential):\n",
    "                if not idx.split('.')[2]=='downsample':\n",
    "                    cfg.append(int(torch.sum(mask)))\n",
    "                    print(torch.sum(mask))\n",
    "                else:\n",
    "                    print('yes')\n",
    "                    print(torch.sum(mask),'**')\n",
    "        #             print(modules[k-2])\n",
    "                cfg_mask.append(mask.clone())\n",
    "                cfg_dict[idx] = mask\n",
    "                print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif idx.split('.')[0] == 'layer3':\n",
    "            if idx.split('.')[-1]!='bn3':\n",
    "                weight_copy = m.weight.data.abs().clone()\n",
    "                mask = weight_copy.gt(thre3).float()\n",
    "                pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "                m.weight.data.mul_(mask)\n",
    "                m.bias.data.mul_(mask)\n",
    "                if not idx.split('.')[2]=='downsample':\n",
    "                    cfg.append(int(torch.sum(mask)))\n",
    "                    print(torch.sum(mask))\n",
    "                else:\n",
    "                    print('yes')\n",
    "                    print(torch.sum(mask),'**')\n",
    "        #             print(modules[k-2])\n",
    "                cfg_mask.append(mask.clone())\n",
    "                cfg_dict[idx] = mask\n",
    "                print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "            \n",
    "        else:\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(0.0).float()\n",
    "#             pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "            try:\n",
    "                if not idx.split('.')[2]=='downsample':\n",
    "                    cfg.append(int(torch.sum(mask)))\n",
    "                    print(torch.sum(mask))\n",
    "                else:\n",
    "                    print('yes')\n",
    "                    print(torch.sum(mask),'**')\n",
    "            except:\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            cfg_dict[idx] = mask\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "    if idx.split('.')[-1]=='mask':\n",
    "        if idx.split('.')[0]!='layer4':\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            y1, i1 = torch.sort(weight_copy[0,:,0,0])\n",
    "            thre_index1 = int(len(y1) * 0.5)\n",
    "            thre4 = y1[thre_index1]\n",
    "            print(thre4)\n",
    "#             print\n",
    "            mask = weight_copy.gt(thre4).float()\n",
    "            pruned = pruned + mask.shape[1] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            cfg.append(int(torch.sum(mask)))\n",
    "            cfg_mask.append(mask.clone())\n",
    "            cfg_dict[idx] = mask[0,:,0,0]\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[1], int(torch.sum(mask))))\n",
    "        else:\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(-1.0).float()\n",
    "#             pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            #m.bias.data.mul_(mask)\n",
    "            cfg_dict[idx] = mask[0,:,0,0]\n",
    "    elif isinstance(m, nn.MaxPool2d):\n",
    "        cfg.append('M')\n",
    "\n",
    "pruned_ratio = pruned/(total1+total2+total3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 27, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(27, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(127, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(24, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(29, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(127, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(28, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(56, 127, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(127, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(127, 102, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(102, 105, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(105, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(105, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(127, 255, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(255, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(5, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(40, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(255, 54, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(54, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(69, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(255, 43, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(43, 93, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(93, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(255, 247, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(247, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(247, 176, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(176, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(255, 511, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(511, 50, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(50, 156, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(156, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(511, 47, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(47, 171, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(171, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(171, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(511, 81, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(81, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(89, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(511, 84, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(84, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(122, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(511, 146, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(146, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(146, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(166, 511, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(511, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(511, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(511, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "# idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bn1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_keys = list(cfg_dict.keys())\n",
    "list_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(newmodel.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bn1', 'layer1.0.bn1', 'layer1.0.bn2', 'layer1.0.mask', 'layer1.1.bn1', 'layer1.1.bn2', 'layer1.1.mask', 'layer1.2.bn1', 'layer1.2.bn2', 'layer1.2.mask', 'layer2.0.bn1', 'layer2.0.bn2', 'layer2.0.mask', 'layer2.1.bn1', 'layer2.1.bn2', 'layer2.1.mask', 'layer2.2.bn1', 'layer2.2.bn2', 'layer2.2.mask', 'layer2.3.bn1', 'layer2.3.bn2', 'layer2.3.mask', 'layer3.0.bn1', 'layer3.0.bn2', 'layer3.0.mask', 'layer3.1.bn1', 'layer3.1.bn2', 'layer3.1.mask', 'layer3.2.bn1', 'layer3.2.bn2', 'layer3.2.mask', 'layer3.3.bn1', 'layer3.3.bn2', 'layer3.3.mask', 'layer3.4.bn1', 'layer3.4.bn2', 'layer3.4.mask', 'layer3.5.bn1', 'layer3.5.bn2', 'layer3.5.mask', 'layer4.0.bn1', 'layer4.0.bn2', 'layer4.0.bn3', 'layer4.0.mask', 'layer4.1.bn1', 'layer4.1.bn2', 'layer4.1.bn3', 'layer4.1.mask', 'layer4.2.bn1', 'layer4.2.bn2', 'layer4.2.bn3', 'layer4.2.mask'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.mask\n",
      "layer2.0.mask\n",
      "layer3.0.mask\n",
      "layer4.0.mask\n",
      "5 16\n"
     ]
    }
   ],
   "source": [
    "cfg_dict_mask_bn = []\n",
    "cfg_dict_mask = []\n",
    "cfg_dict_mask.append(cfg_dict['bn1'].numpy())\n",
    "for i in cfg_dict.keys():\n",
    "    #print(i)\n",
    "    if i.split('.')[-1]=='mask' and i.split('.')[1]=='0':\n",
    "        print(i)\n",
    "        cfg_dict_mask.append(cfg_dict[i].cpu().numpy())\n",
    "    if i.split('.')[-1]=='mask':\n",
    "        cfg_dict_mask_bn.append(cfg_dict[i].cpu().numpy())\n",
    "#((cfg_dict_mask[1]>0)*1).sum()\n",
    "print(len(cfg_dict_mask),len(cfg_dict_mask_bn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keys = list(cfg_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched ************  \n",
      "matched ************ conv1 conv1\n",
      "(3,) (64,)\n",
      "matched ************ bn1 bn1\n",
      "matched ************ relu relu\n",
      "matched ************ maxpool maxpool\n",
      "matched ************ layer1 layer1\n",
      "matched ************ layer1.0 layer1.0\n",
      "matched ************ layer1.0.conv1 layer1.0.conv1\n",
      "(64,) (27,)\n",
      "matched ************ layer1.0.bn1 layer1.0.bn1\n",
      "matched ************ layer1.0.conv2 layer1.0.conv2\n",
      "(27,) (27,)\n",
      "matched ************ layer1.0.bn2 layer1.0.bn2\n",
      "matched ************ layer1.0.conv3 layer1.0.conv3\n",
      "(27,) (127,)\n",
      "matched ************ layer1.0.bn3 layer1.0.bn3\n",
      "matched ************ layer1.0.relu layer1.0.relu\n",
      "unmatched ************ layer1.0.mask layer1.0.downsample\n",
      "matched ************ layer1.0.downsample layer1.0.downsample\n",
      "matched ************ layer1.0.downsample.0 layer1.0.downsample.0\n",
      "unmatched ************ layer1.1 layer1.0.downsample.1\n",
      "matched ************ layer1.1 layer1.1\n",
      "matched ************ layer1.1.conv1 layer1.1.conv1\n",
      "(127,) (24,)\n",
      "matched ************ layer1.1.bn1 layer1.1.bn1\n",
      "matched ************ layer1.1.conv2 layer1.1.conv2\n",
      "(24,) (29,)\n",
      "matched ************ layer1.1.bn2 layer1.1.bn2\n",
      "matched ************ layer1.1.conv3 layer1.1.conv3\n",
      "(29,) (127,)\n",
      "matched ************ layer1.1.bn3 layer1.1.bn3\n",
      "matched ************ layer1.1.relu layer1.1.relu\n",
      "unmatched ************ layer1.1.mask layer1.2\n",
      "matched ************ layer1.2 layer1.2\n",
      "matched ************ layer1.2.conv1 layer1.2.conv1\n",
      "(127,) (28,)\n",
      "matched ************ layer1.2.bn1 layer1.2.bn1\n",
      "matched ************ layer1.2.conv2 layer1.2.conv2\n",
      "(28,) (56,)\n",
      "matched ************ layer1.2.bn2 layer1.2.bn2\n",
      "matched ************ layer1.2.conv3 layer1.2.conv3\n",
      "(56,) (127,)\n",
      "matched ************ layer1.2.bn3 layer1.2.bn3\n",
      "matched ************ layer1.2.relu layer1.2.relu\n",
      "unmatched ************ layer1.2.mask layer2\n",
      "matched ************ layer2 layer2\n",
      "matched ************ layer2.0 layer2.0\n",
      "matched ************ layer2.0.conv1 layer2.0.conv1\n",
      "(127,) (102,)\n",
      "matched ************ layer2.0.bn1 layer2.0.bn1\n",
      "matched ************ layer2.0.conv2 layer2.0.conv2\n",
      "(102,) (105,)\n",
      "matched ************ layer2.0.bn2 layer2.0.bn2\n",
      "matched ************ layer2.0.conv3 layer2.0.conv3\n",
      "(105,) (255,)\n",
      "matched ************ layer2.0.bn3 layer2.0.bn3\n",
      "matched ************ layer2.0.relu layer2.0.relu\n",
      "unmatched ************ layer2.0.mask layer2.0.downsample\n",
      "matched ************ layer2.0.downsample layer2.0.downsample\n",
      "matched ************ layer2.0.downsample.0 layer2.0.downsample.0\n",
      "unmatched ************ layer2.1 layer2.0.downsample.1\n",
      "matched ************ layer2.1 layer2.1\n",
      "matched ************ layer2.1.conv1 layer2.1.conv1\n",
      "(255,) (5,)\n",
      "matched ************ layer2.1.bn1 layer2.1.bn1\n",
      "matched ************ layer2.1.conv2 layer2.1.conv2\n",
      "(5,) (40,)\n",
      "matched ************ layer2.1.bn2 layer2.1.bn2\n",
      "matched ************ layer2.1.conv3 layer2.1.conv3\n",
      "(40,) (255,)\n",
      "matched ************ layer2.1.bn3 layer2.1.bn3\n",
      "matched ************ layer2.1.relu layer2.1.relu\n",
      "unmatched ************ layer2.1.mask layer2.2\n",
      "matched ************ layer2.2 layer2.2\n",
      "matched ************ layer2.2.conv1 layer2.2.conv1\n",
      "(255,) (54,)\n",
      "matched ************ layer2.2.bn1 layer2.2.bn1\n",
      "matched ************ layer2.2.conv2 layer2.2.conv2\n",
      "(54,) (69,)\n",
      "matched ************ layer2.2.bn2 layer2.2.bn2\n",
      "matched ************ layer2.2.conv3 layer2.2.conv3\n",
      "(69,) (255,)\n",
      "matched ************ layer2.2.bn3 layer2.2.bn3\n",
      "matched ************ layer2.2.relu layer2.2.relu\n",
      "unmatched ************ layer2.2.mask layer2.3\n",
      "matched ************ layer2.3 layer2.3\n",
      "matched ************ layer2.3.conv1 layer2.3.conv1\n",
      "(255,) (43,)\n",
      "matched ************ layer2.3.bn1 layer2.3.bn1\n",
      "matched ************ layer2.3.conv2 layer2.3.conv2\n",
      "(43,) (93,)\n",
      "matched ************ layer2.3.bn2 layer2.3.bn2\n",
      "matched ************ layer2.3.conv3 layer2.3.conv3\n",
      "(93,) (255,)\n",
      "matched ************ layer2.3.bn3 layer2.3.bn3\n",
      "matched ************ layer2.3.relu layer2.3.relu\n",
      "unmatched ************ layer2.3.mask layer3\n",
      "matched ************ layer3 layer3\n",
      "matched ************ layer3.0 layer3.0\n",
      "matched ************ layer3.0.conv1 layer3.0.conv1\n",
      "(255,) (247,)\n",
      "matched ************ layer3.0.bn1 layer3.0.bn1\n",
      "matched ************ layer3.0.conv2 layer3.0.conv2\n",
      "(247,) (176,)\n",
      "matched ************ layer3.0.bn2 layer3.0.bn2\n",
      "matched ************ layer3.0.conv3 layer3.0.conv3\n",
      "(176,) (511,)\n",
      "matched ************ layer3.0.bn3 layer3.0.bn3\n",
      "matched ************ layer3.0.relu layer3.0.relu\n",
      "unmatched ************ layer3.0.mask layer3.0.downsample\n",
      "matched ************ layer3.0.downsample layer3.0.downsample\n",
      "matched ************ layer3.0.downsample.0 layer3.0.downsample.0\n",
      "unmatched ************ layer3.1 layer3.0.downsample.1\n",
      "matched ************ layer3.1 layer3.1\n",
      "matched ************ layer3.1.conv1 layer3.1.conv1\n",
      "(511,) (50,)\n",
      "matched ************ layer3.1.bn1 layer3.1.bn1\n",
      "matched ************ layer3.1.conv2 layer3.1.conv2\n",
      "(50,) (156,)\n",
      "matched ************ layer3.1.bn2 layer3.1.bn2\n",
      "matched ************ layer3.1.conv3 layer3.1.conv3\n",
      "(156,) (511,)\n",
      "matched ************ layer3.1.bn3 layer3.1.bn3\n",
      "matched ************ layer3.1.relu layer3.1.relu\n",
      "unmatched ************ layer3.1.mask layer3.2\n",
      "matched ************ layer3.2 layer3.2\n",
      "matched ************ layer3.2.conv1 layer3.2.conv1\n",
      "(511,) (47,)\n",
      "matched ************ layer3.2.bn1 layer3.2.bn1\n",
      "matched ************ layer3.2.conv2 layer3.2.conv2\n",
      "(47,) (171,)\n",
      "matched ************ layer3.2.bn2 layer3.2.bn2\n",
      "matched ************ layer3.2.conv3 layer3.2.conv3\n",
      "(171,) (511,)\n",
      "matched ************ layer3.2.bn3 layer3.2.bn3\n",
      "matched ************ layer3.2.relu layer3.2.relu\n",
      "unmatched ************ layer3.2.mask layer3.3\n",
      "matched ************ layer3.3 layer3.3\n",
      "matched ************ layer3.3.conv1 layer3.3.conv1\n",
      "(511,) (81,)\n",
      "matched ************ layer3.3.bn1 layer3.3.bn1\n",
      "matched ************ layer3.3.conv2 layer3.3.conv2\n",
      "(81,) (89,)\n",
      "matched ************ layer3.3.bn2 layer3.3.bn2\n",
      "matched ************ layer3.3.conv3 layer3.3.conv3\n",
      "(89,) (511,)\n",
      "matched ************ layer3.3.bn3 layer3.3.bn3\n",
      "matched ************ layer3.3.relu layer3.3.relu\n",
      "unmatched ************ layer3.3.mask layer3.4\n",
      "matched ************ layer3.4 layer3.4\n",
      "matched ************ layer3.4.conv1 layer3.4.conv1\n",
      "(511,) (84,)\n",
      "matched ************ layer3.4.bn1 layer3.4.bn1\n",
      "matched ************ layer3.4.conv2 layer3.4.conv2\n",
      "(84,) (122,)\n",
      "matched ************ layer3.4.bn2 layer3.4.bn2\n",
      "matched ************ layer3.4.conv3 layer3.4.conv3\n",
      "(122,) (511,)\n",
      "matched ************ layer3.4.bn3 layer3.4.bn3\n",
      "matched ************ layer3.4.relu layer3.4.relu\n",
      "unmatched ************ layer3.4.mask layer3.5\n",
      "matched ************ layer3.5 layer3.5\n",
      "matched ************ layer3.5.conv1 layer3.5.conv1\n",
      "(511,) (146,)\n",
      "matched ************ layer3.5.bn1 layer3.5.bn1\n",
      "matched ************ layer3.5.conv2 layer3.5.conv2\n",
      "(146,) (166,)\n",
      "matched ************ layer3.5.bn2 layer3.5.bn2\n",
      "matched ************ layer3.5.conv3 layer3.5.conv3\n",
      "(166,) (511,)\n",
      "matched ************ layer3.5.bn3 layer3.5.bn3\n",
      "matched ************ layer3.5.relu layer3.5.relu\n",
      "unmatched ************ layer3.5.mask layer4\n",
      "matched ************ layer4 layer4\n",
      "matched ************ layer4.0 layer4.0\n",
      "matched ************ layer4.0.conv1 layer4.0.conv1\n",
      "matched ************ layer4.0.bn1 layer4.0.bn1\n",
      "matched ************ layer4.0.conv2 layer4.0.conv2\n",
      "matched ************ layer4.0.bn2 layer4.0.bn2\n",
      "matched ************ layer4.0.conv3 layer4.0.conv3\n",
      "matched ************ layer4.0.bn3 layer4.0.bn3\n",
      "matched ************ layer4.0.relu layer4.0.relu\n",
      "unmatched ************ layer4.0.mask layer4.0.downsample\n",
      "matched ************ layer4.0.downsample layer4.0.downsample\n",
      "matched ************ layer4.0.downsample.0 layer4.0.downsample.0\n",
      "unmatched ************ layer4.1 layer4.0.downsample.1\n",
      "matched ************ layer4.1 layer4.1\n",
      "matched ************ layer4.1.conv1 layer4.1.conv1\n",
      "matched ************ layer4.1.bn1 layer4.1.bn1\n",
      "matched ************ layer4.1.conv2 layer4.1.conv2\n",
      "matched ************ layer4.1.bn2 layer4.1.bn2\n",
      "matched ************ layer4.1.conv3 layer4.1.conv3\n",
      "matched ************ layer4.1.bn3 layer4.1.bn3\n",
      "matched ************ layer4.1.relu layer4.1.relu\n",
      "unmatched ************ layer4.1.mask layer4.2\n",
      "matched ************ layer4.2 layer4.2\n",
      "matched ************ layer4.2.conv1 layer4.2.conv1\n",
      "matched ************ layer4.2.bn1 layer4.2.bn1\n",
      "matched ************ layer4.2.conv2 layer4.2.conv2\n",
      "matched ************ layer4.2.bn2 layer4.2.bn2\n",
      "matched ************ layer4.2.conv3 layer4.2.conv3\n",
      "matched ************ layer4.2.bn3 layer4.2.bn3\n",
      "matched ************ layer4.2.relu layer4.2.relu\n",
      "unmatched ************ layer4.2.mask avgpool\n",
      "matched ************ avgpool avgpool\n",
      "matched ************ fc fc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "newmodel = resnet50_child(cfg=cfg)\n",
    "old_modules = list(model.named_modules())\n",
    "new_modules = list(newmodel.named_modules())\n",
    "layer_id_in_cfg = 0\n",
    "start_mask = torch.ones(3)\n",
    "end_mask = cfg_mask[layer_id_in_cfg]\n",
    "count = 0\n",
    "count_conv = -1 \n",
    "first_bn = False\n",
    "k1 = 0\n",
    "k2 = 0\n",
    "while k2!=len(new_modules):\n",
    "    idx_name1,m0 = old_modules[k1]\n",
    "    idx_name2,m1 = new_modules[k2]\n",
    "    if idx_name1!=idx_name2:\n",
    "        if idx_name1.split('.')[-1]=='mask':\n",
    "            k1+=1\n",
    "        else:\n",
    "            k2+=1\n",
    "        print('unmatched','************',idx_name1,idx_name2)\n",
    "        count+=1\n",
    "        \n",
    "    else:\n",
    "        k1+=1\n",
    "        k2+=1\n",
    "        print('matched','************',idx_name1,idx_name2)\n",
    "        if isinstance(m0, nn.BatchNorm2d):\n",
    "            if idx_name1.split('.')[-1]!='bn3':\n",
    "                start_mask = cfg_dict[idx_name1]\n",
    "                idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "                w1 = m0.weight.data[idx0.tolist()].clone()\n",
    "                m1.weight.data = w1.clone()\n",
    "\n",
    "                w2 = m0.bias.data[idx0.tolist()].clone()\n",
    "                m1.bias.data = w2.clone()\n",
    "\n",
    "                #m1.running_mean = m0.running_mean.clone()\n",
    "                #m1.running_var = m0.running_var.clone()\n",
    "        elif isinstance(m0, nn.Conv2d):\n",
    "            if idx_name1.split('.')[0]!='layer4':\n",
    "                if len(idx_name1.split('.'))<=2:\n",
    "\n",
    "                    start_mask = cfg_dict[list_keys[count_conv]]\n",
    "\n",
    "                    end_mask = cfg_dict[list_keys[count_conv+1]]\n",
    "\n",
    "                    if count_conv==-1:\n",
    "                        start_mask = torch.ones(3)\n",
    "\n",
    "                    idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "                    idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "                    print(idx0.shape,idx1.shape)\n",
    "                    w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
    "                    w1 = w1[idx1.tolist(), :, :, :].clone()\n",
    "                    m1.weight.data = w1.clone()\n",
    "                    count_conv+=1\n",
    "\n",
    "                else:\n",
    "                    if idx_name1.split('.')[2]!='downsample':\n",
    "                        start_mask = cfg_dict[list_keys[count_conv]]\n",
    "\n",
    "                        end_mask = cfg_dict[list_keys[count_conv+1]]\n",
    "\n",
    "                        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "                        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n",
    "                        print(idx0.shape,idx1.shape)\n",
    "                        w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
    "                        w1 = w1[idx1.tolist(), :, :, :].clone()\n",
    "                        m1.weight.data = w1.clone()\n",
    "                        count_conv+=1\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       " array([0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1.], dtype=float32),\n",
       " array([1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0.], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 1.], dtype=float32),\n",
       " array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_dict_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1.], dtype=float32),\n",
       " array([0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1.], dtype=float32),\n",
       " array([0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1.], dtype=float32),\n",
       " array([1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0.], dtype=float32),\n",
       " array([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0.], dtype=float32),\n",
       " array([1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0.], dtype=float32),\n",
       " array([1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0.], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 1.], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 1.], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 1.], dtype=float32),\n",
       " array([0., 0., 1., ..., 0., 0., 1.], dtype=float32),\n",
       " array([0., 0., 1., ..., 0., 0., 1.], dtype=float32),\n",
       " array([0., 0., 1., ..., 0., 0., 1.], dtype=float32),\n",
       " array([1., 1., 1., ..., 1., 1., 1.], dtype=float32),\n",
       " array([1., 1., 1., ..., 1., 1., 1.], dtype=float32),\n",
       " array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_dict_mask_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched ************  \n",
      "matched ************ conv1 conv1\n",
      "matched ************ bn1 bn1\n",
      "matched ************ relu relu\n",
      "matched ************ maxpool maxpool\n",
      "matched ************ layer1 layer1\n",
      "matched ************ layer1.0 layer1.0\n",
      "matched ************ layer1.0.conv1 layer1.0.conv1\n",
      "matched ************ layer1.0.bn1 layer1.0.bn1\n",
      "matched ************ layer1.0.conv2 layer1.0.conv2\n",
      "matched ************ layer1.0.bn2 layer1.0.bn2\n",
      "matched ************ layer1.0.conv3 layer1.0.conv3\n",
      "matched ************ layer1.0.bn3 layer1.0.bn3\n",
      "bn3 torch.Size([1, 256, 1, 1])\n",
      "matched ************ layer1.0.relu layer1.0.relu\n",
      "unmatched ************ layer1.0.mask layer1.0.downsample\n",
      "matched ************ layer1.0.downsample layer1.0.downsample\n",
      "matched ************ layer1.0.downsample.0 layer1.0.downsample.0\n",
      "torch.Size([127, 64, 1, 1]) (64,) (127,)\n",
      "(64,) (127,)\n",
      "yes\n",
      "unmatched ************ layer1.0.mask layer1.0.downsample.1\n",
      "matched ************ layer1.1 layer1.1\n",
      "matched ************ layer1.1.conv1 layer1.1.conv1\n",
      "matched ************ layer1.1.bn1 layer1.1.bn1\n",
      "matched ************ layer1.1.conv2 layer1.1.conv2\n",
      "matched ************ layer1.1.bn2 layer1.1.bn2\n",
      "matched ************ layer1.1.conv3 layer1.1.conv3\n",
      "matched ************ layer1.1.bn3 layer1.1.bn3\n",
      "bn3 torch.Size([1, 256, 1, 1])\n",
      "matched ************ layer1.1.relu layer1.1.relu\n",
      "unmatched ************ layer1.1.mask layer1.2\n",
      "matched ************ layer1.2 layer1.2\n",
      "matched ************ layer1.2.conv1 layer1.2.conv1\n",
      "matched ************ layer1.2.bn1 layer1.2.bn1\n",
      "matched ************ layer1.2.conv2 layer1.2.conv2\n",
      "matched ************ layer1.2.bn2 layer1.2.bn2\n",
      "matched ************ layer1.2.conv3 layer1.2.conv3\n",
      "matched ************ layer1.2.bn3 layer1.2.bn3\n",
      "bn3 torch.Size([1, 256, 1, 1])\n",
      "matched ************ layer1.2.relu layer1.2.relu\n",
      "unmatched ************ layer1.2.mask layer2\n",
      "matched ************ layer2 layer2\n",
      "matched ************ layer2.0 layer2.0\n",
      "matched ************ layer2.0.conv1 layer2.0.conv1\n",
      "matched ************ layer2.0.bn1 layer2.0.bn1\n",
      "matched ************ layer2.0.conv2 layer2.0.conv2\n",
      "matched ************ layer2.0.bn2 layer2.0.bn2\n",
      "matched ************ layer2.0.conv3 layer2.0.conv3\n",
      "matched ************ layer2.0.bn3 layer2.0.bn3\n",
      "bn3 torch.Size([1, 512, 1, 1])\n",
      "matched ************ layer2.0.relu layer2.0.relu\n",
      "unmatched ************ layer2.0.mask layer2.0.downsample\n",
      "matched ************ layer2.0.downsample layer2.0.downsample\n",
      "matched ************ layer2.0.downsample.0 layer2.0.downsample.0\n",
      "torch.Size([255, 127, 1, 1]) (127,) (255,)\n",
      "(127,) (255,)\n",
      "yes\n",
      "unmatched ************ layer2.0.mask layer2.0.downsample.1\n",
      "matched ************ layer2.1 layer2.1\n",
      "matched ************ layer2.1.conv1 layer2.1.conv1\n",
      "matched ************ layer2.1.bn1 layer2.1.bn1\n",
      "matched ************ layer2.1.conv2 layer2.1.conv2\n",
      "matched ************ layer2.1.bn2 layer2.1.bn2\n",
      "matched ************ layer2.1.conv3 layer2.1.conv3\n",
      "matched ************ layer2.1.bn3 layer2.1.bn3\n",
      "bn3 torch.Size([1, 512, 1, 1])\n",
      "matched ************ layer2.1.relu layer2.1.relu\n",
      "unmatched ************ layer2.1.mask layer2.2\n",
      "matched ************ layer2.2 layer2.2\n",
      "matched ************ layer2.2.conv1 layer2.2.conv1\n",
      "matched ************ layer2.2.bn1 layer2.2.bn1\n",
      "matched ************ layer2.2.conv2 layer2.2.conv2\n",
      "matched ************ layer2.2.bn2 layer2.2.bn2\n",
      "matched ************ layer2.2.conv3 layer2.2.conv3\n",
      "matched ************ layer2.2.bn3 layer2.2.bn3\n",
      "bn3 torch.Size([1, 512, 1, 1])\n",
      "matched ************ layer2.2.relu layer2.2.relu\n",
      "unmatched ************ layer2.2.mask layer2.3\n",
      "matched ************ layer2.3 layer2.3\n",
      "matched ************ layer2.3.conv1 layer2.3.conv1\n",
      "matched ************ layer2.3.bn1 layer2.3.bn1\n",
      "matched ************ layer2.3.conv2 layer2.3.conv2\n",
      "matched ************ layer2.3.bn2 layer2.3.bn2\n",
      "matched ************ layer2.3.conv3 layer2.3.conv3\n",
      "matched ************ layer2.3.bn3 layer2.3.bn3\n",
      "bn3 torch.Size([1, 512, 1, 1])\n",
      "matched ************ layer2.3.relu layer2.3.relu\n",
      "unmatched ************ layer2.3.mask layer3\n",
      "matched ************ layer3 layer3\n",
      "matched ************ layer3.0 layer3.0\n",
      "matched ************ layer3.0.conv1 layer3.0.conv1\n",
      "matched ************ layer3.0.bn1 layer3.0.bn1\n",
      "matched ************ layer3.0.conv2 layer3.0.conv2\n",
      "matched ************ layer3.0.bn2 layer3.0.bn2\n",
      "matched ************ layer3.0.conv3 layer3.0.conv3\n",
      "matched ************ layer3.0.bn3 layer3.0.bn3\n",
      "bn3 torch.Size([1, 1024, 1, 1])\n",
      "matched ************ layer3.0.relu layer3.0.relu\n",
      "unmatched ************ layer3.0.mask layer3.0.downsample\n",
      "matched ************ layer3.0.downsample layer3.0.downsample\n",
      "matched ************ layer3.0.downsample.0 layer3.0.downsample.0\n",
      "torch.Size([511, 255, 1, 1]) (255,) (511,)\n",
      "(255,) (511,)\n",
      "yes\n",
      "unmatched ************ layer3.0.mask layer3.0.downsample.1\n",
      "matched ************ layer3.1 layer3.1\n",
      "matched ************ layer3.1.conv1 layer3.1.conv1\n",
      "matched ************ layer3.1.bn1 layer3.1.bn1\n",
      "matched ************ layer3.1.conv2 layer3.1.conv2\n",
      "matched ************ layer3.1.bn2 layer3.1.bn2\n",
      "matched ************ layer3.1.conv3 layer3.1.conv3\n",
      "matched ************ layer3.1.bn3 layer3.1.bn3\n",
      "bn3 torch.Size([1, 1024, 1, 1])\n",
      "matched ************ layer3.1.relu layer3.1.relu\n",
      "unmatched ************ layer3.1.mask layer3.2\n",
      "matched ************ layer3.2 layer3.2\n",
      "matched ************ layer3.2.conv1 layer3.2.conv1\n",
      "matched ************ layer3.2.bn1 layer3.2.bn1\n",
      "matched ************ layer3.2.conv2 layer3.2.conv2\n",
      "matched ************ layer3.2.bn2 layer3.2.bn2\n",
      "matched ************ layer3.2.conv3 layer3.2.conv3\n",
      "matched ************ layer3.2.bn3 layer3.2.bn3\n",
      "bn3 torch.Size([1, 1024, 1, 1])\n",
      "matched ************ layer3.2.relu layer3.2.relu\n",
      "unmatched ************ layer3.2.mask layer3.3\n",
      "matched ************ layer3.3 layer3.3\n",
      "matched ************ layer3.3.conv1 layer3.3.conv1\n",
      "matched ************ layer3.3.bn1 layer3.3.bn1\n",
      "matched ************ layer3.3.conv2 layer3.3.conv2\n",
      "matched ************ layer3.3.bn2 layer3.3.bn2\n",
      "matched ************ layer3.3.conv3 layer3.3.conv3\n",
      "matched ************ layer3.3.bn3 layer3.3.bn3\n",
      "bn3 torch.Size([1, 1024, 1, 1])\n",
      "matched ************ layer3.3.relu layer3.3.relu\n",
      "unmatched ************ layer3.3.mask layer3.4\n",
      "matched ************ layer3.4 layer3.4\n",
      "matched ************ layer3.4.conv1 layer3.4.conv1\n",
      "matched ************ layer3.4.bn1 layer3.4.bn1\n",
      "matched ************ layer3.4.conv2 layer3.4.conv2\n",
      "matched ************ layer3.4.bn2 layer3.4.bn2\n",
      "matched ************ layer3.4.conv3 layer3.4.conv3\n",
      "matched ************ layer3.4.bn3 layer3.4.bn3\n",
      "bn3 torch.Size([1, 1024, 1, 1])\n",
      "matched ************ layer3.4.relu layer3.4.relu\n",
      "unmatched ************ layer3.4.mask layer3.5\n",
      "matched ************ layer3.5 layer3.5\n",
      "matched ************ layer3.5.conv1 layer3.5.conv1\n",
      "matched ************ layer3.5.bn1 layer3.5.bn1\n",
      "matched ************ layer3.5.conv2 layer3.5.conv2\n",
      "matched ************ layer3.5.bn2 layer3.5.bn2\n",
      "matched ************ layer3.5.conv3 layer3.5.conv3\n",
      "matched ************ layer3.5.bn3 layer3.5.bn3\n",
      "bn3 torch.Size([1, 1024, 1, 1])\n",
      "matched ************ layer3.5.relu layer3.5.relu\n",
      "unmatched ************ layer3.5.mask layer4\n",
      "matched ************ layer4 layer4\n",
      "matched ************ layer4.0 layer4.0\n",
      "matched ************ layer4.0.conv1 layer4.0.conv1\n",
      "matched ************ layer4.0.bn1 layer4.0.bn1\n",
      "matched ************ layer4.0.conv2 layer4.0.conv2\n",
      "matched ************ layer4.0.bn2 layer4.0.bn2\n",
      "matched ************ layer4.0.conv3 layer4.0.conv3\n",
      "matched ************ layer4.0.bn3 layer4.0.bn3\n",
      "bn3 torch.Size([1, 2048, 1, 1])\n",
      "matched ************ layer4.0.relu layer4.0.relu\n",
      "unmatched ************ layer4.0.mask layer4.0.downsample\n",
      "matched ************ layer4.0.downsample layer4.0.downsample\n",
      "matched ************ layer4.0.downsample.0 layer4.0.downsample.0\n",
      "torch.Size([2048, 511, 1, 1]) (511,) (2048,)\n",
      "(511,) (2048,)\n",
      "yes\n",
      "unmatched ************ layer4.0.mask layer4.0.downsample.1\n",
      "matched ************ layer4.1 layer4.1\n",
      "matched ************ layer4.1.conv1 layer4.1.conv1\n",
      "matched ************ layer4.1.bn1 layer4.1.bn1\n",
      "matched ************ layer4.1.conv2 layer4.1.conv2\n",
      "matched ************ layer4.1.bn2 layer4.1.bn2\n",
      "matched ************ layer4.1.conv3 layer4.1.conv3\n",
      "matched ************ layer4.1.bn3 layer4.1.bn3\n",
      "bn3 torch.Size([1, 2048, 1, 1])\n",
      "matched ************ layer4.1.relu layer4.1.relu\n",
      "unmatched ************ layer4.1.mask layer4.2\n",
      "matched ************ layer4.2 layer4.2\n",
      "matched ************ layer4.2.conv1 layer4.2.conv1\n",
      "matched ************ layer4.2.bn1 layer4.2.bn1\n",
      "matched ************ layer4.2.conv2 layer4.2.conv2\n",
      "matched ************ layer4.2.bn2 layer4.2.bn2\n",
      "matched ************ layer4.2.conv3 layer4.2.conv3\n",
      "matched ************ layer4.2.bn3 layer4.2.bn3\n",
      "bn3 torch.Size([1, 2048, 1, 1])\n",
      "matched ************ layer4.2.relu layer4.2.relu\n",
      "unmatched ************ layer4.2.mask avgpool\n",
      "matched ************ avgpool avgpool\n",
      "matched ************ fc fc\n"
     ]
    }
   ],
   "source": [
    "k1 = 0\n",
    "k2 = 0\n",
    "count = 0 #conv\n",
    "count_db = 1  #downsample\n",
    "count_bn = 0   #batchnorm\n",
    "while k2!=len(new_modules):\n",
    "    idx_name1,m0 = old_modules[k1]\n",
    "    idx_name2,m1 = new_modules[k2]\n",
    "    if idx_name1!=idx_name2:\n",
    "        if idx_name1.split('.')[-1]=='mask':\n",
    "            k1+=1\n",
    "        else:\n",
    "            if idx_name2.split('.')[-1]=='1' and idx_name2.split('.')[-2]=='downsample':\n",
    "                print('yes')\n",
    "                idx_name1,m0 = old_modules[k1-3]\n",
    "                start_mask = cfg_dict_mask[count_db]\n",
    "                idx0 = np.squeeze(np.argwhere(np.asarray(start_mask)))\n",
    "#                 print(m0.weight.shape)\n",
    "#                 print(idx0)\n",
    "                \n",
    "                w1 = m0.weight.data[:,idx0.tolist(),:,:].clone()\n",
    "                m1.weight.data = w1[0,:,0,0].clone()\n",
    "                \n",
    "                count_db+=1\n",
    "            k2+=1\n",
    "                \n",
    "        print('unmatched','************',idx_name1,idx_name2)\n",
    "        #count+=1\n",
    "            \n",
    "    else:\n",
    "        k1+=1\n",
    "        k2+=1\n",
    "        print('matched','************',idx_name1,idx_name2)\n",
    "        if isinstance(m0,nn.BatchNorm2d):\n",
    "            if idx_name1.split('.')[-1]=='bn3':\n",
    "                start_mask = cfg_dict_mask_bn[count_bn]\n",
    "                idx0 = np.squeeze(np.argwhere(np.asarray(start_mask)))\n",
    "                idx_name1,m0 = old_modules[k1+1]\n",
    "                print('bn3',m0.weight.shape)\n",
    "                w1 = m0.weight.data[:,idx0.tolist(),:,:].clone()\n",
    "                m1.weight.data = w1[0,:,0,0].clone()\n",
    "                \n",
    "                count_bn+=1\n",
    "                \n",
    "        if isinstance(m0,nn.Conv2d):\n",
    "            if len(idx_name1.split('.'))>2:\n",
    "                if idx_name1.split('.')[2]=='downsample':\n",
    "                    start_mask = cfg_dict_mask[count]\n",
    "                    end_mask = cfg_dict_mask[count+1]\n",
    "                    idx0 = np.squeeze(np.argwhere(np.asarray(start_mask)))\n",
    "                    idx1 = np.squeeze(np.argwhere(np.asarray(end_mask)))\n",
    "                    print(m1.weight.data.shape,idx0.shape,idx1.shape)\n",
    "                    print(idx0.shape,idx1.shape)\n",
    "                    w1 = m0.weight.data[:, idx0.tolist(), :, :].clone()\n",
    "                    w1 = w1[idx1.tolist(), :, :, :].clone()\n",
    "                    m1.weight.data = w1.clone()\n",
    "                    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k1 = 0\n",
    "k2 = 0\n",
    "count = 0 #conv\n",
    "count_db = 1  #downsample\n",
    "count_bn = 0   #batchnorm\n",
    "while k2!=len(new_modules):\n",
    "    idx_name1,m0 = old_modules[k1]\n",
    "    idx_name2,m1 = new_modules[k2]\n",
    "    if idx_name1!=idx_name2:\n",
    "        if idx_name1.split('.')[-1]=='mask':\n",
    "            k1+=1\n",
    "        else:\n",
    "#             if idx_name2.split('.')[-1]=='1':\n",
    "#                 print('yes')\n",
    "#                 idx_name_db,m0_db = old_modules[k1-3]\n",
    "#                 print(idx_name_db)\n",
    "#                 start_mask_db = cfg_dict_mask[count_db]\n",
    "#                 idx0_db = np.squeeze(np.argwhere(np.asarray(start_mask_db.cpu().numpy())))\n",
    "#                 w1_db = m0_db.weight.data[idx0_db.tolist()].clone()\n",
    "#                 m1.weight.data = w1_db.clone()\n",
    "                \n",
    "#                 count_db+=1\n",
    "            k2+=1\n",
    "                \n",
    "        print('unmatched','************',idx_name1,idx_name2)\n",
    "        #count+=1\n",
    "            \n",
    "    else:\n",
    "        k1+=1\n",
    "        k2+=1\n",
    "        print('matched','************',idx_name1,idx_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'cfg': cfg, 'state_dict': newmodel.state_dict()},f'/workspace/tracking_datasets/pruned_ckpts/dimp50_shared_mask_wo_bn/pruned_50p.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = resnet50_child(cfg=cfg)\n",
    "x.load_state_dict(torch.load('/workspace/tracking_datasets/pruned_ckpts/dimp50_shared_mask_wo_bn/pruned_50p.pth')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.rand((2,3,224,224))\n",
    "x(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['feature_extractor.conv1.weight', 'feature_extractor.bn1.weight', 'feature_extractor.bn1.bias', 'feature_extractor.bn1.running_mean', 'feature_extractor.bn1.running_var', 'feature_extractor.bn1.num_batches_tracked', 'feature_extractor.layer1.0.conv1.weight', 'feature_extractor.layer1.0.bn1.weight', 'feature_extractor.layer1.0.bn1.bias', 'feature_extractor.layer1.0.bn1.running_mean', 'feature_extractor.layer1.0.bn1.running_var', 'feature_extractor.layer1.0.bn1.num_batches_tracked', 'feature_extractor.layer1.0.conv2.weight', 'feature_extractor.layer1.0.bn2.weight', 'feature_extractor.layer1.0.bn2.bias', 'feature_extractor.layer1.0.bn2.running_mean', 'feature_extractor.layer1.0.bn2.running_var', 'feature_extractor.layer1.0.bn2.num_batches_tracked', 'feature_extractor.layer1.0.conv3.weight', 'feature_extractor.layer1.0.bn3.weight', 'feature_extractor.layer1.0.bn3.bias', 'feature_extractor.layer1.0.bn3.running_mean', 'feature_extractor.layer1.0.bn3.running_var', 'feature_extractor.layer1.0.bn3.num_batches_tracked', 'feature_extractor.layer1.0.mask.weight', 'feature_extractor.layer1.0.downsample.0.weight', 'feature_extractor.layer1.1.conv1.weight', 'feature_extractor.layer1.1.bn1.weight', 'feature_extractor.layer1.1.bn1.bias', 'feature_extractor.layer1.1.bn1.running_mean', 'feature_extractor.layer1.1.bn1.running_var', 'feature_extractor.layer1.1.bn1.num_batches_tracked', 'feature_extractor.layer1.1.conv2.weight', 'feature_extractor.layer1.1.bn2.weight', 'feature_extractor.layer1.1.bn2.bias', 'feature_extractor.layer1.1.bn2.running_mean', 'feature_extractor.layer1.1.bn2.running_var', 'feature_extractor.layer1.1.bn2.num_batches_tracked', 'feature_extractor.layer1.1.conv3.weight', 'feature_extractor.layer1.1.bn3.weight', 'feature_extractor.layer1.1.bn3.bias', 'feature_extractor.layer1.1.bn3.running_mean', 'feature_extractor.layer1.1.bn3.running_var', 'feature_extractor.layer1.1.bn3.num_batches_tracked', 'feature_extractor.layer1.1.mask.weight', 'feature_extractor.layer1.2.conv1.weight', 'feature_extractor.layer1.2.bn1.weight', 'feature_extractor.layer1.2.bn1.bias', 'feature_extractor.layer1.2.bn1.running_mean', 'feature_extractor.layer1.2.bn1.running_var', 'feature_extractor.layer1.2.bn1.num_batches_tracked', 'feature_extractor.layer1.2.conv2.weight', 'feature_extractor.layer1.2.bn2.weight', 'feature_extractor.layer1.2.bn2.bias', 'feature_extractor.layer1.2.bn2.running_mean', 'feature_extractor.layer1.2.bn2.running_var', 'feature_extractor.layer1.2.bn2.num_batches_tracked', 'feature_extractor.layer1.2.conv3.weight', 'feature_extractor.layer1.2.bn3.weight', 'feature_extractor.layer1.2.bn3.bias', 'feature_extractor.layer1.2.bn3.running_mean', 'feature_extractor.layer1.2.bn3.running_var', 'feature_extractor.layer1.2.bn3.num_batches_tracked', 'feature_extractor.layer1.2.mask.weight', 'feature_extractor.layer2.0.conv1.weight', 'feature_extractor.layer2.0.bn1.weight', 'feature_extractor.layer2.0.bn1.bias', 'feature_extractor.layer2.0.bn1.running_mean', 'feature_extractor.layer2.0.bn1.running_var', 'feature_extractor.layer2.0.bn1.num_batches_tracked', 'feature_extractor.layer2.0.conv2.weight', 'feature_extractor.layer2.0.bn2.weight', 'feature_extractor.layer2.0.bn2.bias', 'feature_extractor.layer2.0.bn2.running_mean', 'feature_extractor.layer2.0.bn2.running_var', 'feature_extractor.layer2.0.bn2.num_batches_tracked', 'feature_extractor.layer2.0.conv3.weight', 'feature_extractor.layer2.0.bn3.weight', 'feature_extractor.layer2.0.bn3.bias', 'feature_extractor.layer2.0.bn3.running_mean', 'feature_extractor.layer2.0.bn3.running_var', 'feature_extractor.layer2.0.bn3.num_batches_tracked', 'feature_extractor.layer2.0.mask.weight', 'feature_extractor.layer2.0.downsample.0.weight', 'feature_extractor.layer2.1.conv1.weight', 'feature_extractor.layer2.1.bn1.weight', 'feature_extractor.layer2.1.bn1.bias', 'feature_extractor.layer2.1.bn1.running_mean', 'feature_extractor.layer2.1.bn1.running_var', 'feature_extractor.layer2.1.bn1.num_batches_tracked', 'feature_extractor.layer2.1.conv2.weight', 'feature_extractor.layer2.1.bn2.weight', 'feature_extractor.layer2.1.bn2.bias', 'feature_extractor.layer2.1.bn2.running_mean', 'feature_extractor.layer2.1.bn2.running_var', 'feature_extractor.layer2.1.bn2.num_batches_tracked', 'feature_extractor.layer2.1.conv3.weight', 'feature_extractor.layer2.1.bn3.weight', 'feature_extractor.layer2.1.bn3.bias', 'feature_extractor.layer2.1.bn3.running_mean', 'feature_extractor.layer2.1.bn3.running_var', 'feature_extractor.layer2.1.bn3.num_batches_tracked', 'feature_extractor.layer2.1.mask.weight', 'feature_extractor.layer2.2.conv1.weight', 'feature_extractor.layer2.2.bn1.weight', 'feature_extractor.layer2.2.bn1.bias', 'feature_extractor.layer2.2.bn1.running_mean', 'feature_extractor.layer2.2.bn1.running_var', 'feature_extractor.layer2.2.bn1.num_batches_tracked', 'feature_extractor.layer2.2.conv2.weight', 'feature_extractor.layer2.2.bn2.weight', 'feature_extractor.layer2.2.bn2.bias', 'feature_extractor.layer2.2.bn2.running_mean', 'feature_extractor.layer2.2.bn2.running_var', 'feature_extractor.layer2.2.bn2.num_batches_tracked', 'feature_extractor.layer2.2.conv3.weight', 'feature_extractor.layer2.2.bn3.weight', 'feature_extractor.layer2.2.bn3.bias', 'feature_extractor.layer2.2.bn3.running_mean', 'feature_extractor.layer2.2.bn3.running_var', 'feature_extractor.layer2.2.bn3.num_batches_tracked', 'feature_extractor.layer2.2.mask.weight', 'feature_extractor.layer2.3.conv1.weight', 'feature_extractor.layer2.3.bn1.weight', 'feature_extractor.layer2.3.bn1.bias', 'feature_extractor.layer2.3.bn1.running_mean', 'feature_extractor.layer2.3.bn1.running_var', 'feature_extractor.layer2.3.bn1.num_batches_tracked', 'feature_extractor.layer2.3.conv2.weight', 'feature_extractor.layer2.3.bn2.weight', 'feature_extractor.layer2.3.bn2.bias', 'feature_extractor.layer2.3.bn2.running_mean', 'feature_extractor.layer2.3.bn2.running_var', 'feature_extractor.layer2.3.bn2.num_batches_tracked', 'feature_extractor.layer2.3.conv3.weight', 'feature_extractor.layer2.3.bn3.weight', 'feature_extractor.layer2.3.bn3.bias', 'feature_extractor.layer2.3.bn3.running_mean', 'feature_extractor.layer2.3.bn3.running_var', 'feature_extractor.layer2.3.bn3.num_batches_tracked', 'feature_extractor.layer2.3.mask.weight', 'feature_extractor.layer3.0.conv1.weight', 'feature_extractor.layer3.0.bn1.weight', 'feature_extractor.layer3.0.bn1.bias', 'feature_extractor.layer3.0.bn1.running_mean', 'feature_extractor.layer3.0.bn1.running_var', 'feature_extractor.layer3.0.bn1.num_batches_tracked', 'feature_extractor.layer3.0.conv2.weight', 'feature_extractor.layer3.0.bn2.weight', 'feature_extractor.layer3.0.bn2.bias', 'feature_extractor.layer3.0.bn2.running_mean', 'feature_extractor.layer3.0.bn2.running_var', 'feature_extractor.layer3.0.bn2.num_batches_tracked', 'feature_extractor.layer3.0.conv3.weight', 'feature_extractor.layer3.0.bn3.weight', 'feature_extractor.layer3.0.bn3.bias', 'feature_extractor.layer3.0.bn3.running_mean', 'feature_extractor.layer3.0.bn3.running_var', 'feature_extractor.layer3.0.bn3.num_batches_tracked', 'feature_extractor.layer3.0.mask.weight', 'feature_extractor.layer3.0.downsample.0.weight', 'feature_extractor.layer3.1.conv1.weight', 'feature_extractor.layer3.1.bn1.weight', 'feature_extractor.layer3.1.bn1.bias', 'feature_extractor.layer3.1.bn1.running_mean', 'feature_extractor.layer3.1.bn1.running_var', 'feature_extractor.layer3.1.bn1.num_batches_tracked', 'feature_extractor.layer3.1.conv2.weight', 'feature_extractor.layer3.1.bn2.weight', 'feature_extractor.layer3.1.bn2.bias', 'feature_extractor.layer3.1.bn2.running_mean', 'feature_extractor.layer3.1.bn2.running_var', 'feature_extractor.layer3.1.bn2.num_batches_tracked', 'feature_extractor.layer3.1.conv3.weight', 'feature_extractor.layer3.1.bn3.weight', 'feature_extractor.layer3.1.bn3.bias', 'feature_extractor.layer3.1.bn3.running_mean', 'feature_extractor.layer3.1.bn3.running_var', 'feature_extractor.layer3.1.bn3.num_batches_tracked', 'feature_extractor.layer3.1.mask.weight', 'feature_extractor.layer3.2.conv1.weight', 'feature_extractor.layer3.2.bn1.weight', 'feature_extractor.layer3.2.bn1.bias', 'feature_extractor.layer3.2.bn1.running_mean', 'feature_extractor.layer3.2.bn1.running_var', 'feature_extractor.layer3.2.bn1.num_batches_tracked', 'feature_extractor.layer3.2.conv2.weight', 'feature_extractor.layer3.2.bn2.weight', 'feature_extractor.layer3.2.bn2.bias', 'feature_extractor.layer3.2.bn2.running_mean', 'feature_extractor.layer3.2.bn2.running_var', 'feature_extractor.layer3.2.bn2.num_batches_tracked', 'feature_extractor.layer3.2.conv3.weight', 'feature_extractor.layer3.2.bn3.weight', 'feature_extractor.layer3.2.bn3.bias', 'feature_extractor.layer3.2.bn3.running_mean', 'feature_extractor.layer3.2.bn3.running_var', 'feature_extractor.layer3.2.bn3.num_batches_tracked', 'feature_extractor.layer3.2.mask.weight', 'feature_extractor.layer3.3.conv1.weight', 'feature_extractor.layer3.3.bn1.weight', 'feature_extractor.layer3.3.bn1.bias', 'feature_extractor.layer3.3.bn1.running_mean', 'feature_extractor.layer3.3.bn1.running_var', 'feature_extractor.layer3.3.bn1.num_batches_tracked', 'feature_extractor.layer3.3.conv2.weight', 'feature_extractor.layer3.3.bn2.weight', 'feature_extractor.layer3.3.bn2.bias', 'feature_extractor.layer3.3.bn2.running_mean', 'feature_extractor.layer3.3.bn2.running_var', 'feature_extractor.layer3.3.bn2.num_batches_tracked', 'feature_extractor.layer3.3.conv3.weight', 'feature_extractor.layer3.3.bn3.weight', 'feature_extractor.layer3.3.bn3.bias', 'feature_extractor.layer3.3.bn3.running_mean', 'feature_extractor.layer3.3.bn3.running_var', 'feature_extractor.layer3.3.bn3.num_batches_tracked', 'feature_extractor.layer3.3.mask.weight', 'feature_extractor.layer3.4.conv1.weight', 'feature_extractor.layer3.4.bn1.weight', 'feature_extractor.layer3.4.bn1.bias', 'feature_extractor.layer3.4.bn1.running_mean', 'feature_extractor.layer3.4.bn1.running_var', 'feature_extractor.layer3.4.bn1.num_batches_tracked', 'feature_extractor.layer3.4.conv2.weight', 'feature_extractor.layer3.4.bn2.weight', 'feature_extractor.layer3.4.bn2.bias', 'feature_extractor.layer3.4.bn2.running_mean', 'feature_extractor.layer3.4.bn2.running_var', 'feature_extractor.layer3.4.bn2.num_batches_tracked', 'feature_extractor.layer3.4.conv3.weight', 'feature_extractor.layer3.4.bn3.weight', 'feature_extractor.layer3.4.bn3.bias', 'feature_extractor.layer3.4.bn3.running_mean', 'feature_extractor.layer3.4.bn3.running_var', 'feature_extractor.layer3.4.bn3.num_batches_tracked', 'feature_extractor.layer3.4.mask.weight', 'feature_extractor.layer3.5.conv1.weight', 'feature_extractor.layer3.5.bn1.weight', 'feature_extractor.layer3.5.bn1.bias', 'feature_extractor.layer3.5.bn1.running_mean', 'feature_extractor.layer3.5.bn1.running_var', 'feature_extractor.layer3.5.bn1.num_batches_tracked', 'feature_extractor.layer3.5.conv2.weight', 'feature_extractor.layer3.5.bn2.weight', 'feature_extractor.layer3.5.bn2.bias', 'feature_extractor.layer3.5.bn2.running_mean', 'feature_extractor.layer3.5.bn2.running_var', 'feature_extractor.layer3.5.bn2.num_batches_tracked', 'feature_extractor.layer3.5.conv3.weight', 'feature_extractor.layer3.5.bn3.weight', 'feature_extractor.layer3.5.bn3.bias', 'feature_extractor.layer3.5.bn3.running_mean', 'feature_extractor.layer3.5.bn3.running_var', 'feature_extractor.layer3.5.bn3.num_batches_tracked', 'feature_extractor.layer3.5.mask.weight', 'feature_extractor.layer4.0.conv1.weight', 'feature_extractor.layer4.0.bn1.weight', 'feature_extractor.layer4.0.bn1.bias', 'feature_extractor.layer4.0.bn1.running_mean', 'feature_extractor.layer4.0.bn1.running_var', 'feature_extractor.layer4.0.bn1.num_batches_tracked', 'feature_extractor.layer4.0.conv2.weight', 'feature_extractor.layer4.0.bn2.weight', 'feature_extractor.layer4.0.bn2.bias', 'feature_extractor.layer4.0.bn2.running_mean', 'feature_extractor.layer4.0.bn2.running_var', 'feature_extractor.layer4.0.bn2.num_batches_tracked', 'feature_extractor.layer4.0.conv3.weight', 'feature_extractor.layer4.0.bn3.weight', 'feature_extractor.layer4.0.bn3.bias', 'feature_extractor.layer4.0.bn3.running_mean', 'feature_extractor.layer4.0.bn3.running_var', 'feature_extractor.layer4.0.bn3.num_batches_tracked', 'feature_extractor.layer4.0.mask.weight', 'feature_extractor.layer4.0.downsample.0.weight', 'feature_extractor.layer4.1.conv1.weight', 'feature_extractor.layer4.1.bn1.weight', 'feature_extractor.layer4.1.bn1.bias', 'feature_extractor.layer4.1.bn1.running_mean', 'feature_extractor.layer4.1.bn1.running_var', 'feature_extractor.layer4.1.bn1.num_batches_tracked', 'feature_extractor.layer4.1.conv2.weight', 'feature_extractor.layer4.1.bn2.weight', 'feature_extractor.layer4.1.bn2.bias', 'feature_extractor.layer4.1.bn2.running_mean', 'feature_extractor.layer4.1.bn2.running_var', 'feature_extractor.layer4.1.bn2.num_batches_tracked', 'feature_extractor.layer4.1.conv3.weight', 'feature_extractor.layer4.1.bn3.weight', 'feature_extractor.layer4.1.bn3.bias', 'feature_extractor.layer4.1.bn3.running_mean', 'feature_extractor.layer4.1.bn3.running_var', 'feature_extractor.layer4.1.bn3.num_batches_tracked', 'feature_extractor.layer4.1.mask.weight', 'feature_extractor.layer4.2.conv1.weight', 'feature_extractor.layer4.2.bn1.weight', 'feature_extractor.layer4.2.bn1.bias', 'feature_extractor.layer4.2.bn1.running_mean', 'feature_extractor.layer4.2.bn1.running_var', 'feature_extractor.layer4.2.bn1.num_batches_tracked', 'feature_extractor.layer4.2.conv2.weight', 'feature_extractor.layer4.2.bn2.weight', 'feature_extractor.layer4.2.bn2.bias', 'feature_extractor.layer4.2.bn2.running_mean', 'feature_extractor.layer4.2.bn2.running_var', 'feature_extractor.layer4.2.bn2.num_batches_tracked', 'feature_extractor.layer4.2.conv3.weight', 'feature_extractor.layer4.2.bn3.weight', 'feature_extractor.layer4.2.bn3.bias', 'feature_extractor.layer4.2.bn3.running_mean', 'feature_extractor.layer4.2.bn3.running_var', 'feature_extractor.layer4.2.bn3.num_batches_tracked', 'feature_extractor.layer4.2.mask.weight', 'feature_extractor.fc.weight', 'feature_extractor.fc.bias', 'classifier.filter_initializer.filter_conv.weight', 'classifier.filter_initializer.filter_conv.bias', 'classifier.filter_optimizer.log_step_length', 'classifier.filter_optimizer.filter_reg', 'classifier.filter_optimizer.label_map_predictor.weight', 'classifier.filter_optimizer.target_mask_predictor.0.weight', 'classifier.filter_optimizer.spatial_weight_predictor.weight', 'classifier.feature_extractor.0.weight', 'bb_regressor.conv3_1r.0.weight', 'bb_regressor.conv3_1r.0.bias', 'bb_regressor.conv3_1r.1.weight', 'bb_regressor.conv3_1r.1.bias', 'bb_regressor.conv3_1r.1.running_mean', 'bb_regressor.conv3_1r.1.running_var', 'bb_regressor.conv3_1r.1.num_batches_tracked', 'bb_regressor.conv3_1t.0.weight', 'bb_regressor.conv3_1t.0.bias', 'bb_regressor.conv3_1t.1.weight', 'bb_regressor.conv3_1t.1.bias', 'bb_regressor.conv3_1t.1.running_mean', 'bb_regressor.conv3_1t.1.running_var', 'bb_regressor.conv3_1t.1.num_batches_tracked', 'bb_regressor.conv3_2t.0.weight', 'bb_regressor.conv3_2t.0.bias', 'bb_regressor.conv3_2t.1.weight', 'bb_regressor.conv3_2t.1.bias', 'bb_regressor.conv3_2t.1.running_mean', 'bb_regressor.conv3_2t.1.running_var', 'bb_regressor.conv3_2t.1.num_batches_tracked', 'bb_regressor.fc3_1r.0.weight', 'bb_regressor.fc3_1r.0.bias', 'bb_regressor.fc3_1r.1.weight', 'bb_regressor.fc3_1r.1.bias', 'bb_regressor.fc3_1r.1.running_mean', 'bb_regressor.fc3_1r.1.running_var', 'bb_regressor.fc3_1r.1.num_batches_tracked', 'bb_regressor.conv4_1r.0.weight', 'bb_regressor.conv4_1r.0.bias', 'bb_regressor.conv4_1r.1.weight', 'bb_regressor.conv4_1r.1.bias', 'bb_regressor.conv4_1r.1.running_mean', 'bb_regressor.conv4_1r.1.running_var', 'bb_regressor.conv4_1r.1.num_batches_tracked', 'bb_regressor.conv4_1t.0.weight', 'bb_regressor.conv4_1t.0.bias', 'bb_regressor.conv4_1t.1.weight', 'bb_regressor.conv4_1t.1.bias', 'bb_regressor.conv4_1t.1.running_mean', 'bb_regressor.conv4_1t.1.running_var', 'bb_regressor.conv4_1t.1.num_batches_tracked', 'bb_regressor.conv4_2t.0.weight', 'bb_regressor.conv4_2t.0.bias', 'bb_regressor.conv4_2t.1.weight', 'bb_regressor.conv4_2t.1.bias', 'bb_regressor.conv4_2t.1.running_mean', 'bb_regressor.conv4_2t.1.running_var', 'bb_regressor.conv4_2t.1.num_batches_tracked', 'bb_regressor.fc34_3r.0.weight', 'bb_regressor.fc34_3r.0.bias', 'bb_regressor.fc34_3r.1.weight', 'bb_regressor.fc34_3r.1.bias', 'bb_regressor.fc34_3r.1.running_mean', 'bb_regressor.fc34_3r.1.running_var', 'bb_regressor.fc34_3r.1.num_batches_tracked', 'bb_regressor.fc34_4r.0.weight', 'bb_regressor.fc34_4r.0.bias', 'bb_regressor.fc34_4r.1.weight', 'bb_regressor.fc34_4r.1.bias', 'bb_regressor.fc34_4r.1.running_mean', 'bb_regressor.fc34_4r.1.running_var', 'bb_regressor.fc34_4r.1.num_batches_tracked', 'bb_regressor.fc3_rt.linear.weight', 'bb_regressor.fc3_rt.linear.bias', 'bb_regressor.fc3_rt.bn.weight', 'bb_regressor.fc3_rt.bn.bias', 'bb_regressor.fc3_rt.bn.running_mean', 'bb_regressor.fc3_rt.bn.running_var', 'bb_regressor.fc3_rt.bn.num_batches_tracked', 'bb_regressor.fc4_rt.linear.weight', 'bb_regressor.fc4_rt.linear.bias', 'bb_regressor.fc4_rt.bn.weight', 'bb_regressor.fc4_rt.bn.bias', 'bb_regressor.fc4_rt.bn.running_mean', 'bb_regressor.fc4_rt.bn.running_var', 'bb_regressor.fc4_rt.bn.num_batches_tracked', 'bb_regressor.iou_predictor.weight', 'bb_regressor.iou_predictor.bias'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.0119e-09]],\n",
       "\n",
       "         [[ 2.0645e-02]],\n",
       "\n",
       "         [[ 5.3493e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0813e-08]],\n",
       "\n",
       "         [[-1.4123e-02]],\n",
       "\n",
       "         [[-4.2867e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 5.7415e-09]],\n",
       "\n",
       "         [[ 1.0545e-02]],\n",
       "\n",
       "         [[ 7.7597e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.3281e-09]],\n",
       "\n",
       "         [[ 1.5247e-02]],\n",
       "\n",
       "         [[ 4.9836e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.2345e-09]],\n",
       "\n",
       "         [[ 7.8737e-03]],\n",
       "\n",
       "         [[-1.9548e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.2814e-09]],\n",
       "\n",
       "         [[ 3.7478e-03]],\n",
       "\n",
       "         [[-1.3946e-02]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 6.1314e-09]],\n",
       "\n",
       "         [[-1.8629e-03]],\n",
       "\n",
       "         [[-1.2791e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2589e-08]],\n",
       "\n",
       "         [[-1.6471e-03]],\n",
       "\n",
       "         [[ 5.1506e-03]]],\n",
       "\n",
       "\n",
       "        [[[-7.4988e-09]],\n",
       "\n",
       "         [[-1.2011e-02]],\n",
       "\n",
       "         [[ 1.0461e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.6372e-09]],\n",
       "\n",
       "         [[-3.8683e-02]],\n",
       "\n",
       "         [[ 6.3428e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4321e-09]],\n",
       "\n",
       "         [[-2.6719e-03]],\n",
       "\n",
       "         [[-2.0555e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.1837e-09]],\n",
       "\n",
       "         [[ 3.5551e-02]],\n",
       "\n",
       "         [[-1.2461e-01]]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['feature_extractor.layer1.0.conv3.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 7.7597e-03]],\n",
       "\n",
       "         [[-3.7214e-03]],\n",
       "\n",
       "         [[ 1.1199e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.3117e-03]],\n",
       "\n",
       "         [[ 8.0621e-03]],\n",
       "\n",
       "         [[ 4.9836e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.9548e-03]],\n",
       "\n",
       "         [[ 9.0840e-03]],\n",
       "\n",
       "         [[-4.5280e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.0070e-03]],\n",
       "\n",
       "         [[ 1.1128e-02]],\n",
       "\n",
       "         [[-1.3946e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 9.0324e-02]],\n",
       "\n",
       "         [[ 7.1718e-02]],\n",
       "\n",
       "         [[ 2.8351e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.1181e-01]],\n",
       "\n",
       "         [[-1.2754e-02]],\n",
       "\n",
       "         [[-1.7638e-02]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-1.2791e-02]],\n",
       "\n",
       "         [[-8.6347e-03]],\n",
       "\n",
       "         [[ 1.1124e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0229e-04]],\n",
       "\n",
       "         [[-3.3172e-03]],\n",
       "\n",
       "         [[ 5.1506e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0461e-01]],\n",
       "\n",
       "         [[ 9.2669e-02]],\n",
       "\n",
       "         [[-2.7954e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-6.8950e-03]],\n",
       "\n",
       "         [[-1.3025e-02]],\n",
       "\n",
       "         [[ 6.3428e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.0555e-02]],\n",
       "\n",
       "         [[-5.6770e-02]],\n",
       "\n",
       "         [[ 5.7762e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.8537e-02]],\n",
       "\n",
       "         [[-5.6907e-04]],\n",
       "\n",
       "         [[-1.2461e-01]]]], requires_grad=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.layer1[0].conv3.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 0\n",
    "k2 = 0\n",
    "count = 0 \n",
    "while k2!=len(new_modules):\n",
    "    idx1,m0 = old_modules[k1]\n",
    "    idx2,m1 = new_modules[k2]\n",
    "    if idx1!=idx2:\n",
    "        if idx1.split('.')[-1]=='mask':\n",
    "            k1+=1\n",
    "        else:\n",
    "            k2+=1\n",
    "        print('unmatched','************',idx1,idx2)\n",
    "        count+=1\n",
    "        \n",
    "    else:\n",
    "        k1+=1\n",
    "        k2+=1\n",
    "        print('matched','************',idx1,idx2)\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "newmodel = resnet50(cfg=cfg)\n",
    "old_modules = list(model.modules())\n",
    "new_modules = list(newmodel.modules())\n",
    "layer_id_in_cfg = 0\n",
    "start_mask = torch.ones(3)\n",
    "end_mask = cfg_mask[layer_id_in_cfg]\n",
    "conv_count = 0\n",
    "first_bn = False\n",
    "\n",
    "for layer_id in range(len(old_modules)):\n",
    "    m0 = old_modules[layer_id]\n",
    "    m1 = new_modules[layer_id]\n",
    "    if isinstance(m0, nn.BatchNorm2d):\n",
    "        if first_bn==False:\n",
    "            first_bn=True\n",
    "            m1.weight.data = m0.weight.data.clone()\n",
    "            m1.bias.data = m0.bias.data.clone()\n",
    "            m1.running_mean = m0.running_mean.clone()\n",
    "            m1.running_var = m0.running_var.clone()\n",
    "            layer_id_in_cfg += 1\n",
    "            start_mask = end_mask.clone()\n",
    "            if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n",
    "                end_mask = cfg_mask[layer_id_in_cfg]\n",
    "            continue\n",
    "        \n",
    "        \n",
    " \n",
    "    elif isinstance(m0, nn.Linear):\n",
    "        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n",
    "        if idx0.size == 1:\n",
    "            idx0 = np.resize(idx0, (1,))\n",
    "\n",
    "        m1.weight.data = m0.weight.data[:, idx0].clone()\n",
    "        m1.bias.data = m0.bias.data.clone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
