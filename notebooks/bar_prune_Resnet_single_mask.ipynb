{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove the unnecessary filters from a CNN pruned using main.py, then compare the converted net with the previous.\n",
    "While main.py allows to determine which filters can be removed, those remain in the network's architecture. The\n",
    "current script creates a new, lightweight architecture from the result of main.py.\n",
    "\n",
    "NOTE: The arguments passed to this script are parsed in main.py (i.e. a dataset choice must be made).\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import VITALabAI.project.bar.model.resnet as resnet\n",
    "# from VITALabAI.project.bar.model.layers import SparseConvConfig\n",
    "# # from VITALabAI.project.bar.main import ClassificationTraining\n",
    "\n",
    "\n",
    "class IdentityModule(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "def convert_resnet(net, insert_identity_modules=False):\n",
    "    \"\"\"Convert a ResNetCifar module (in place)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        net: the mutated net\n",
    "    \"\"\"\n",
    "    \n",
    "    net.conv1, net.bn1 = convert_conv_bn(net.conv1, net.bn1, torch.ones(3).byte(), (cfg_dict['bn1']>0))\n",
    "    in_gates = torch.ones(net.conv1.out_channels).byte()\n",
    "\n",
    "    clean_res = True\n",
    "    net.layer1, in_gates = convert_layer(net.layer1, in_gates, insert_identity_modules, clean_res, layer_name = 'layer1')\n",
    "    net.layer2, in_gates = convert_layer(net.layer2, in_gates, insert_identity_modules, clean_res, layer_name = 'layer2')\n",
    "    net.layer3, in_gates = convert_layer(net.layer3, in_gates, insert_identity_modules, clean_res, layer_name = 'layer3')\n",
    "    net.layer4, in_gates = convert_layer(net.layer4, in_gates, insert_identity_modules, clean_res, layer_name = 'layer4')\n",
    "\n",
    "\n",
    "    if clean_res:\n",
    "        net.fc = convert_fc_head(net.fc, in_gates)\n",
    "    else:\n",
    "        net.fc = resnet.InwardPrunedLinear(convert_fc_head(net.fc, in_gates), mask2i(in_gates))\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def convert_layer(layer_module, in_gates, insert_identity_modules, clean_res, layer_name =None):\n",
    "    \"\"\"Convert a ResnetCifar layer (in place)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        layer_module: a nn.Sequential\n",
    "        in_gates: mask\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        layer_module: mutated layer_module\n",
    "        in_gates: ajusted mask\n",
    "    \"\"\"\n",
    "\n",
    "    previous_layer_gates = in_gates\n",
    "\n",
    "    new_blocks = []\n",
    "    for block_num, block in enumerate(layer_module):\n",
    "        new_block, in_gates = convert_block(block, in_gates, block_name = layer_name+'.'+str(block_num))\n",
    "        if new_block is None:\n",
    "            if insert_identity_modules:\n",
    "                new_blocks.append(IdentityModule())\n",
    "        else:\n",
    "            new_blocks.append(new_block)\n",
    "\n",
    "    # Remove unused residual features\n",
    "    if clean_res:\n",
    "        print()\n",
    "        cur_layer_gates = in_gates\n",
    "        for block in new_blocks:\n",
    "            if isinstance(block, IdentityModule):\n",
    "                continue\n",
    "            clean_block(block, previous_layer_gates, cur_layer_gates)  # in-place\n",
    "\n",
    "    layer_module = nn.Sequential(*new_blocks)\n",
    "    return layer_module, in_gates\n",
    "\n",
    "\n",
    "def clean_block(mixed_block, previous_layer_alivef, cur_layer_alivef):\n",
    "    \"\"\"Remove unused res features (operates in-place)\"\"\"\n",
    "\n",
    "    def clean_indices(idx, alive_mask=cur_layer_alivef):\n",
    "        mask = i2mask(idx, alive_mask)\n",
    "        mask = mask[mask2i(alive_mask)]\n",
    "        return mask2i(mask)\n",
    "\n",
    "    if mixed_block.f_res is None:\n",
    "        mixed_block.in_idx = clean_indices(mixed_block.in_idx)\n",
    "    else:\n",
    "        mixed_block.in_idx = clean_indices(mixed_block.in_idx, alive_mask=previous_layer_alivef)\n",
    "        mixed_block.res_size = cur_layer_alivef.sum().item()\n",
    "        print('DOWNS ----- Res size: ', mixed_block.res_size)\n",
    "        mixed_block.res_idx = clean_indices(mixed_block.res_idx)\n",
    "        print('Res:  ', len(mixed_block.res_idx))\n",
    "    mixed_block.delta_idx = clean_indices(mixed_block.delta_idx)\n",
    "\n",
    "    print('In:   ', len(mixed_block.in_idx))\n",
    "    print('Delta:', len(mixed_block.delta_idx))\n",
    "\n",
    "\n",
    "def convert_block(block_module, in_gates, block_name = None):\n",
    "    \"\"\"Convert a Basic Resnet block (in place)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        block_module: a BasicBlock\n",
    "        in_gates: received mask\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        block_module: mutated block\n",
    "        in_gates: out_gates of this block (in_gates for next block)\n",
    "    \"\"\"\n",
    "\n",
    "#     assert not hasattr(block_module, 'conv3')  # must be basic block\n",
    "\n",
    "    b1_gates = (cfg_dict[f'{block_name}.bn1']>0)   # get_gates(block_module.bn1)\n",
    "    b2_gates = (cfg_dict[f'{block_name}.bn2']>0)\n",
    "    b3_gates = (cfg_dict[f'{block_name}.bn3']>0)\n",
    "   \n",
    "\n",
    "    delta_branch_is_pruned = b1_gates.sum().item() == 0 or b2_gates.sum().item() == 0 or b3_gates.sum().item() == 0\n",
    "    \n",
    "    # Delta branch\n",
    "    if not delta_branch_is_pruned:\n",
    "        block_module.conv1, block_module.bn1 = convert_conv_bn(block_module.conv1, block_module.bn1, in_gates, b1_gates)\n",
    "        block_module.conv2, block_module.bn2 = convert_conv_bn(block_module.conv2, block_module.bn2, b1_gates, b2_gates)\n",
    "        block_module.conv3, block_module.bn3 = convert_conv_bn(block_module.conv3, block_module.bn3, b2_gates, b3_gates)\n",
    "\n",
    "\n",
    "    if block_module.downsample is not None:\n",
    "        ds_gates = (cfg_dict[f'{block_name}.downsample.1']>0) # get_gates(block_module.downsample[1])\n",
    "        ds_conv, ds_bn = convert_conv_bn(block_module.downsample[0], block_module.downsample[1], in_gates, ds_gates)\n",
    "        ds_module = nn.Sequential(ds_conv, ds_bn)\n",
    "\n",
    "        if delta_branch_is_pruned:\n",
    "            mixed_block = MixedBlock(f_delta=None, delta_idx=None,\n",
    "                                            f_res=ds_module,\n",
    "                                            in_idx=mask2i(in_gates),\n",
    "                                            res_idx=mask2i(ds_gates),\n",
    "                                            res_size=len(b3_gates))\n",
    "        else:\n",
    "            block_module.downsample = ds_module\n",
    "            mixed_block = MixedBlock.from_bottleneck(block_module,\n",
    "                                                       delta_idx=mask2i(b3_gates),\n",
    "                                                       in_idx=mask2i(in_gates),\n",
    "                                                       res_idx=mask2i(ds_gates),\n",
    "                                                       res_size=len(b3_gates))\n",
    "        in_gates = elementwise_or(ds_gates, b3_gates)\n",
    "    else:\n",
    "        if delta_branch_is_pruned:\n",
    "            mixed_block = None\n",
    "        else:\n",
    "            mixed_block = MixedBlock.from_bottleneck(block_module,\n",
    "                                                       delta_idx=mask2i(b3_gates),\n",
    "                                                       in_idx=mask2i(in_gates))\n",
    "        in_gates = elementwise_or(in_gates, b3_gates)\n",
    "\n",
    "    return mixed_block, in_gates\n",
    "\n",
    "\n",
    "def convert_conv_bn(conv_module, bn_module, in_gates, out_gates):\n",
    "    in_indices = mask2i(in_gates)  # indices of kept features\n",
    "    out_indices = mask2i(out_gates)\n",
    "\n",
    "    # Keep the good ones\n",
    "    new_conv_w = conv_module.weight.data[out_indices][:, in_indices]\n",
    "\n",
    "    new_conv = make_conv(new_conv_w, from_module=conv_module)\n",
    "    new_bn = convert_bn(bn_module, out_indices)\n",
    "\n",
    "    new_conv.out_idx = out_indices\n",
    "    \n",
    "    return new_conv, new_bn\n",
    "\n",
    "\n",
    "def convert_fc_head(fc_module, in_gates):\n",
    "    \"\"\"Convert a the final FC module of the net\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        fc_module: a nn.Linear with weight tensor of size (out_f, in_f)\n",
    "        in_gates: binary vector or list of size in_f\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        fc_module: mutated module\n",
    "    \"\"\"\n",
    "\n",
    "    in_indices = mask2i(in_gates)\n",
    "    new_weight_tensor = fc_module.weight.data[:, in_indices]\n",
    "    return make_fc(new_weight_tensor, from_module=fc_module)\n",
    "\n",
    "\n",
    "def convert_bn(bn_module, out_indices):\n",
    "#     z = bn_module.get_gates(stochastic=False)\n",
    "    new_weight = bn_module.weight.data[out_indices] # * z[out_indices]\n",
    "    new_bias = bn_module.bias.data[out_indices] # * z[out_indices]\n",
    "\n",
    "    new_bn_module = nn.BatchNorm2d(len(new_weight))\n",
    "    new_bn_module.weight.data.copy_(new_weight)\n",
    "    new_bn_module.bias.data.copy_(new_bias)\n",
    "    new_bn_module.running_mean.copy_(bn_module.running_mean[out_indices])\n",
    "    new_bn_module.running_var.copy_(bn_module.running_var[out_indices])\n",
    "\n",
    "    new_bn_module.out_idx = out_indices\n",
    "\n",
    "    return new_bn_module\n",
    "\n",
    "\n",
    "def make_bn(bn_module, kept_indices):\n",
    "    new_bn_module = nn.BatchNorm2d(len(kept_indices))\n",
    "    new_bn_module.weight.data.copy_(bn_module.weight.data[kept_indices])\n",
    "    new_bn_module.bias.data.copy_(bn_module.bias.data[kept_indices])\n",
    "    new_bn_module.running_mean.copy_(bn_module.running_mean[kept_indices])\n",
    "    new_bn_module.running_var.copy_(bn_module.running_var[kept_indices])\n",
    "\n",
    "    if hasattr(bn_module, 'out_idx'):\n",
    "        new_bn_module.out_idx = bn_module.out_idx[kept_indices]\n",
    "    else:\n",
    "        new_bn_module.out_idx = kept_indices\n",
    "\n",
    "    return new_bn_module\n",
    "\n",
    "\n",
    "def make_conv(weight_tensor, from_module):\n",
    "    # NOTE: No bias\n",
    "\n",
    "    # New weight size\n",
    "    in_channels = weight_tensor.size(1)\n",
    "    out_channels = weight_tensor.size(0)\n",
    "\n",
    "    # Other params\n",
    "    kernel_size = from_module.kernel_size\n",
    "    stride = from_module.stride\n",
    "    padding = from_module.padding\n",
    "\n",
    "    conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    conv.weight.data.copy_(weight_tensor)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def make_fc(weight_tensor, from_module):\n",
    "    in_features = weight_tensor.size(1)\n",
    "    out_features = weight_tensor.size(0)\n",
    "    fc = nn.Linear(in_features, out_features)\n",
    "    fc.weight.data.copy_(weight_tensor)\n",
    "    fc.bias.data.copy_(from_module.bias.data)\n",
    "    return fc\n",
    "\n",
    "def elementwise_or(a, b):\n",
    "    return (a + b) > 0\n",
    "\n",
    "\n",
    "def mask2i(mask):\n",
    "#     assert mask.dtype == torch.uint8\n",
    "    return mask.nonzero().view(-1)  # Note: do not use .squeeze() because single item becomes a scalar instead of 1-vec\n",
    "\n",
    "\n",
    "def i2mask(i, from_tensor):\n",
    "    x = torch.zeros_like(from_tensor)\n",
    "    x[i] = 1\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedBlock(nn.Module):\n",
    "    def __init__(self, f_delta, delta_idx, in_idx, f_res=None, res_idx=None, res_size=None):\n",
    "        super(MixedBlock, self).__init__()\n",
    "        self.f_delta = f_delta\n",
    "        self.delta_idx = delta_idx\n",
    "        self.in_idx = in_idx\n",
    "        self.f_res = f_res\n",
    "        self.res_idx = res_idx\n",
    "        self.res_size = res_size\n",
    "        self.activ = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.res_scatter_idx = None\n",
    "        self.delta_scatter_idx = None\n",
    "\n",
    "        if f_delta is None:\n",
    "            self.forward = self.forward_without_delta\n",
    "        else:\n",
    "            self.forward = self.forward_with_delta\n",
    "\n",
    "    def scatter_features(self, idx, src, final_size):\n",
    "        if self.res_scatter_idx is None or self.res_scatter_idx.size(0) != src.size(0):\n",
    "            scatter_idx = idx.new_empty(*src.size())\n",
    "            scatter_idx.copy_(idx[None, :, None, None])\n",
    "            self.res_scatter_idx = scatter_idx\n",
    "\n",
    "        x = torch.zeros(src.size(0), final_size, src.size(2), src.size(3)).to(src)\n",
    "        x.scatter_(dim=1, index=self.res_scatter_idx, src=src)\n",
    "        return x\n",
    "\n",
    "    def scatter_add_features(self, dst, idx, src):\n",
    "        if self.delta_scatter_idx is None or self.delta_scatter_idx.size(0) != src.size(0):\n",
    "            scatter_idx = idx.new_empty(*src.size())\n",
    "            scatter_idx.copy_(idx[None, :, None, None])\n",
    "            self.delta_scatter_idx = scatter_idx\n",
    "\n",
    "        dst.scatter_add_(dim=1, index=self.delta_scatter_idx, src=src)\n",
    "\n",
    "    def forward_with_delta(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "\n",
    "        if self.f_res is None:\n",
    "            x_alive = x.index_select(dim=1, index=self.in_idx)\n",
    "            delta = self.f_delta.forward(x_alive)  # 3x3 conv\n",
    "        else:\n",
    "            delta = self.f_delta.forward(x)  # 3x3 conv\n",
    "\n",
    "            res = self.f_res.forward(x)  # 1x1 conv\n",
    "            x = self.scatter_features(self.res_idx, res, self.res_size)\n",
    "\n",
    "        self.scatter_add_features(x, self.delta_idx, delta)\n",
    "        \n",
    "        return self.activ(x)\n",
    "\n",
    "    def forward_without_delta(self, x):\n",
    "        res = self.f_res.forward(x)  # 1x1 conv\n",
    "        x = self.scatter_features(self.res_idx, res, self.res_size)\n",
    "\n",
    "        return self.activ(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_basic(block, delta_idx, in_idx, res_idx=None, res_size=None):\n",
    "        f_delta = nn.Sequential(\n",
    "            block.conv1,\n",
    "            block.bn1,\n",
    "            block.activ,  # nn.ReLU(inplace=True)\n",
    "            block.conv2,\n",
    "            block.bn2\n",
    "        )\n",
    "        return MixedBlock(f_delta, delta_idx, in_idx, block.downsample, res_idx, res_size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_bottleneck(block, delta_idx, in_idx, res_idx=None, res_size=None):\n",
    "        f_delta = nn.Sequential(\n",
    "            block.conv1,\n",
    "            block.bn1,\n",
    "            block.relu,  # nn.ReLU(inplace=True)\n",
    "            block.conv2,\n",
    "            block.bn2,\n",
    "            block.relu,\n",
    "            block.conv3,\n",
    "            block.bn3\n",
    "        )\n",
    "        return MixedBlock(f_delta, delta_idx, in_idx, block.downsample, res_idx, res_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/pytracking\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/pytracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50_single_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/pytracking\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/pytracking\n",
    "from ltr.models.backbone.resnet import resnet50\n",
    "from ltr.models.backbone.resnet_single_mask import resnet50_single_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.admin.loading import torch_load_legacy\n",
    "ckpt = torch_load_legacy('/workspace/tracking_datasets/saved_ckpts/ltr/dimp/sparse/dimp50_single_mask/DiMPnet_ep0038.pth.tar')['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['initializer.filter_conv.weight', 'initializer.filter_conv.bias', 'optimizer.log_step_length', 'optimizer.filter_reg', 'optimizer.label_map_predictor.weight', 'optimizer.target_mask_predictor.0.weight', 'optimizer.spatial_weight_predictor.weight', '_extractor.0.weight', '_1r.0.weight', '_1r.0.bias', '_1r.1.weight', '_1r.1.bias', '_1r.1.running_mean', '_1r.1.running_var', '_1r.1.num_batches_tracked', '_1t.0.weight', '_1t.0.bias', '_1t.1.weight', '_1t.1.bias', '_1t.1.running_mean', '_1t.1.running_var', '_1t.1.num_batches_tracked', '_2t.0.weight', '_2t.0.bias', '_2t.1.weight', '_2t.1.bias', '_2t.1.running_mean', '_2t.1.running_var', '_2t.1.num_batches_tracked', 'r.0.weight', 'r.0.bias', 'r.1.weight', 'r.1.bias', 'r.1.running_mean', 'r.1.running_var', 'r.1.num_batches_tracked', '3r.0.weight', '3r.0.bias', '3r.1.weight', '3r.1.bias', '3r.1.running_mean', '3r.1.running_var', '3r.1.num_batches_tracked', '4r.0.weight', '4r.0.bias', '4r.1.weight', '4r.1.bias', '4r.1.running_mean', '4r.1.running_var', '4r.1.num_batches_tracked', 't.linear.weight', 't.linear.bias', 't.bn.weight', 't.bn.bias', 't.bn.running_mean', 't.bn.running_var', 't.bn.num_batches_tracked', 'redictor.weight', 'redictor.bias'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "new_state = OrderedDict()\n",
    "\n",
    "for key, value in ckpt.items():\n",
    "    key = key[18:] # remove `module.`\n",
    "    new_state[key] = value\n",
    "model.load_state_dict(new_state, strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum((model.layer1[0].mask.weight>0.5).cpu().numpy()*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.9439e-01]],\n",
       "\n",
       "         [[ 5.2811e-01]],\n",
       "\n",
       "         [[ 5.0189e-01]],\n",
       "\n",
       "         [[ 5.0036e-01]],\n",
       "\n",
       "         [[ 4.2392e-01]],\n",
       "\n",
       "         [[ 4.8955e-01]],\n",
       "\n",
       "         [[ 4.4509e-01]],\n",
       "\n",
       "         [[ 5.1082e-01]],\n",
       "\n",
       "         [[ 8.0642e-05]],\n",
       "\n",
       "         [[-6.3515e-04]],\n",
       "\n",
       "         [[ 4.1840e-01]],\n",
       "\n",
       "         [[ 4.9996e-01]],\n",
       "\n",
       "         [[ 5.0042e-01]],\n",
       "\n",
       "         [[ 5.1869e-01]],\n",
       "\n",
       "         [[ 5.0854e-01]],\n",
       "\n",
       "         [[ 4.9412e-01]],\n",
       "\n",
       "         [[ 3.1977e-01]],\n",
       "\n",
       "         [[ 5.1078e-01]],\n",
       "\n",
       "         [[ 5.0340e-01]],\n",
       "\n",
       "         [[-3.0990e-04]],\n",
       "\n",
       "         [[ 3.7836e-01]],\n",
       "\n",
       "         [[ 5.0641e-01]],\n",
       "\n",
       "         [[-4.7648e-06]],\n",
       "\n",
       "         [[ 1.8034e-05]],\n",
       "\n",
       "         [[ 5.0227e-01]],\n",
       "\n",
       "         [[ 9.6952e-05]],\n",
       "\n",
       "         [[ 5.1169e-01]],\n",
       "\n",
       "         [[ 5.1010e-01]],\n",
       "\n",
       "         [[ 5.2279e-01]],\n",
       "\n",
       "         [[ 5.0660e-01]],\n",
       "\n",
       "         [[ 5.1267e-01]],\n",
       "\n",
       "         [[ 5.1161e-01]],\n",
       "\n",
       "         [[ 5.0470e-01]],\n",
       "\n",
       "         [[ 4.8641e-01]],\n",
       "\n",
       "         [[ 2.0751e-04]],\n",
       "\n",
       "         [[-4.3775e-04]],\n",
       "\n",
       "         [[-1.1354e-03]],\n",
       "\n",
       "         [[ 5.1256e-01]],\n",
       "\n",
       "         [[ 4.9929e-01]],\n",
       "\n",
       "         [[ 5.0677e-01]],\n",
       "\n",
       "         [[ 4.8437e-01]],\n",
       "\n",
       "         [[ 5.0371e-01]],\n",
       "\n",
       "         [[ 5.0807e-01]],\n",
       "\n",
       "         [[ 5.0464e-01]],\n",
       "\n",
       "         [[ 5.0523e-01]],\n",
       "\n",
       "         [[-2.4896e-05]],\n",
       "\n",
       "         [[-9.4467e-04]],\n",
       "\n",
       "         [[ 5.0288e-01]],\n",
       "\n",
       "         [[ 4.9926e-01]],\n",
       "\n",
       "         [[ 1.2647e-06]],\n",
       "\n",
       "         [[ 4.8804e-01]],\n",
       "\n",
       "         [[ 5.0464e-01]],\n",
       "\n",
       "         [[-2.0520e-05]],\n",
       "\n",
       "         [[ 5.0475e-01]],\n",
       "\n",
       "         [[ 5.0620e-01]],\n",
       "\n",
       "         [[ 1.7304e-05]],\n",
       "\n",
       "         [[ 1.2301e-01]],\n",
       "\n",
       "         [[ 5.0881e-01]],\n",
       "\n",
       "         [[ 5.1096e-01]],\n",
       "\n",
       "         [[ 5.0199e-01]],\n",
       "\n",
       "         [[ 1.3077e-01]],\n",
       "\n",
       "         [[ 5.1923e-01]],\n",
       "\n",
       "         [[ 1.7596e-05]],\n",
       "\n",
       "         [[ 4.9655e-01]],\n",
       "\n",
       "         [[ 2.0548e-06]],\n",
       "\n",
       "         [[ 5.2657e-01]],\n",
       "\n",
       "         [[ 5.0435e-01]],\n",
       "\n",
       "         [[ 5.1073e-01]],\n",
       "\n",
       "         [[ 5.0608e-01]],\n",
       "\n",
       "         [[ 4.5251e-01]],\n",
       "\n",
       "         [[ 4.9065e-01]],\n",
       "\n",
       "         [[ 4.8522e-01]],\n",
       "\n",
       "         [[ 9.7970e-06]],\n",
       "\n",
       "         [[ 4.9933e-01]],\n",
       "\n",
       "         [[ 5.0805e-01]],\n",
       "\n",
       "         [[ 5.0364e-01]],\n",
       "\n",
       "         [[ 5.7476e-06]],\n",
       "\n",
       "         [[ 4.9275e-01]],\n",
       "\n",
       "         [[ 5.1492e-01]],\n",
       "\n",
       "         [[ 5.0480e-01]],\n",
       "\n",
       "         [[ 3.4514e-01]],\n",
       "\n",
       "         [[ 5.0844e-01]],\n",
       "\n",
       "         [[ 5.4121e-02]],\n",
       "\n",
       "         [[-1.6698e-05]],\n",
       "\n",
       "         [[ 5.0238e-01]],\n",
       "\n",
       "         [[ 5.0661e-01]],\n",
       "\n",
       "         [[ 4.3006e-01]],\n",
       "\n",
       "         [[-1.8951e-05]],\n",
       "\n",
       "         [[ 5.1608e-01]],\n",
       "\n",
       "         [[ 2.1422e-05]],\n",
       "\n",
       "         [[ 5.0969e-01]],\n",
       "\n",
       "         [[ 5.0147e-01]],\n",
       "\n",
       "         [[ 5.0188e-01]],\n",
       "\n",
       "         [[ 2.1895e-01]],\n",
       "\n",
       "         [[ 4.3483e-01]],\n",
       "\n",
       "         [[ 2.5639e-01]],\n",
       "\n",
       "         [[ 5.1442e-01]],\n",
       "\n",
       "         [[ 5.0840e-01]],\n",
       "\n",
       "         [[ 5.0637e-01]],\n",
       "\n",
       "         [[ 3.8528e-01]],\n",
       "\n",
       "         [[ 5.1023e-01]],\n",
       "\n",
       "         [[ 2.2193e-06]],\n",
       "\n",
       "         [[ 4.9214e-01]],\n",
       "\n",
       "         [[-3.0569e-04]],\n",
       "\n",
       "         [[ 4.9486e-01]],\n",
       "\n",
       "         [[ 4.7366e-01]],\n",
       "\n",
       "         [[ 4.9578e-01]],\n",
       "\n",
       "         [[ 4.4456e-01]],\n",
       "\n",
       "         [[ 5.1932e-01]],\n",
       "\n",
       "         [[ 5.0578e-01]],\n",
       "\n",
       "         [[ 4.8475e-01]],\n",
       "\n",
       "         [[ 5.0677e-01]],\n",
       "\n",
       "         [[ 5.1559e-01]],\n",
       "\n",
       "         [[ 5.0428e-01]],\n",
       "\n",
       "         [[ 5.1875e-01]],\n",
       "\n",
       "         [[ 5.0476e-01]],\n",
       "\n",
       "         [[ 4.9712e-01]],\n",
       "\n",
       "         [[-5.0254e-05]],\n",
       "\n",
       "         [[ 5.1410e-01]],\n",
       "\n",
       "         [[ 5.0933e-01]],\n",
       "\n",
       "         [[-9.1785e-05]],\n",
       "\n",
       "         [[ 5.0802e-01]],\n",
       "\n",
       "         [[ 4.9933e-01]],\n",
       "\n",
       "         [[ 7.5456e-06]],\n",
       "\n",
       "         [[ 4.9323e-01]],\n",
       "\n",
       "         [[ 5.1736e-01]],\n",
       "\n",
       "         [[ 4.9294e-01]],\n",
       "\n",
       "         [[ 7.0537e-06]],\n",
       "\n",
       "         [[ 5.0043e-01]],\n",
       "\n",
       "         [[ 5.1288e-01]],\n",
       "\n",
       "         [[ 5.1217e-01]],\n",
       "\n",
       "         [[ 5.0981e-01]],\n",
       "\n",
       "         [[ 5.0754e-01]],\n",
       "\n",
       "         [[ 4.2905e-05]],\n",
       "\n",
       "         [[-1.1033e-05]],\n",
       "\n",
       "         [[ 1.9238e-01]],\n",
       "\n",
       "         [[ 5.0273e-01]],\n",
       "\n",
       "         [[ 5.0351e-01]],\n",
       "\n",
       "         [[ 4.9983e-01]],\n",
       "\n",
       "         [[ 4.9170e-01]],\n",
       "\n",
       "         [[ 4.9728e-01]],\n",
       "\n",
       "         [[ 4.9984e-01]],\n",
       "\n",
       "         [[ 5.0972e-01]],\n",
       "\n",
       "         [[ 1.1742e-04]],\n",
       "\n",
       "         [[ 4.9598e-01]],\n",
       "\n",
       "         [[ 4.8788e-01]],\n",
       "\n",
       "         [[ 5.0059e-01]],\n",
       "\n",
       "         [[ 5.0531e-01]],\n",
       "\n",
       "         [[ 4.9643e-01]],\n",
       "\n",
       "         [[ 4.6847e-01]],\n",
       "\n",
       "         [[ 5.0321e-01]],\n",
       "\n",
       "         [[ 5.0024e-01]],\n",
       "\n",
       "         [[ 4.9707e-01]],\n",
       "\n",
       "         [[ 4.9552e-01]],\n",
       "\n",
       "         [[ 5.0810e-01]],\n",
       "\n",
       "         [[ 4.9202e-01]],\n",
       "\n",
       "         [[ 5.0617e-01]],\n",
       "\n",
       "         [[ 5.0482e-01]],\n",
       "\n",
       "         [[ 4.9744e-01]],\n",
       "\n",
       "         [[ 7.0335e-05]],\n",
       "\n",
       "         [[ 5.2080e-01]],\n",
       "\n",
       "         [[ 5.0646e-01]],\n",
       "\n",
       "         [[ 4.9411e-01]],\n",
       "\n",
       "         [[ 5.1754e-01]],\n",
       "\n",
       "         [[ 4.9302e-01]],\n",
       "\n",
       "         [[ 3.0246e-01]],\n",
       "\n",
       "         [[ 5.0751e-01]],\n",
       "\n",
       "         [[ 1.2767e-01]],\n",
       "\n",
       "         [[ 4.9602e-01]],\n",
       "\n",
       "         [[ 4.9916e-01]],\n",
       "\n",
       "         [[ 4.8848e-01]],\n",
       "\n",
       "         [[ 3.5147e-01]],\n",
       "\n",
       "         [[ 4.9081e-01]],\n",
       "\n",
       "         [[ 4.2686e-01]],\n",
       "\n",
       "         [[ 1.1182e-04]],\n",
       "\n",
       "         [[ 5.0978e-01]],\n",
       "\n",
       "         [[ 5.1069e-01]],\n",
       "\n",
       "         [[ 5.3751e-01]],\n",
       "\n",
       "         [[ 5.0589e-01]],\n",
       "\n",
       "         [[ 4.8349e-01]],\n",
       "\n",
       "         [[ 5.0793e-01]],\n",
       "\n",
       "         [[ 5.1224e-01]],\n",
       "\n",
       "         [[ 4.9077e-01]],\n",
       "\n",
       "         [[ 5.1811e-01]],\n",
       "\n",
       "         [[ 5.1690e-01]],\n",
       "\n",
       "         [[ 4.0855e-06]],\n",
       "\n",
       "         [[ 5.5042e-02]],\n",
       "\n",
       "         [[ 4.9487e-01]],\n",
       "\n",
       "         [[ 5.0035e-01]],\n",
       "\n",
       "         [[ 5.1684e-01]],\n",
       "\n",
       "         [[ 4.9635e-01]],\n",
       "\n",
       "         [[ 5.1828e-01]],\n",
       "\n",
       "         [[ 5.0573e-01]],\n",
       "\n",
       "         [[ 5.0159e-01]],\n",
       "\n",
       "         [[ 4.7627e-01]],\n",
       "\n",
       "         [[-1.5425e-03]],\n",
       "\n",
       "         [[ 4.9891e-01]],\n",
       "\n",
       "         [[ 5.0782e-01]],\n",
       "\n",
       "         [[ 5.1174e-01]],\n",
       "\n",
       "         [[ 6.8366e-07]],\n",
       "\n",
       "         [[ 4.9514e-01]],\n",
       "\n",
       "         [[ 5.1078e-01]],\n",
       "\n",
       "         [[ 1.2755e-06]],\n",
       "\n",
       "         [[ 5.0534e-01]],\n",
       "\n",
       "         [[ 3.6315e-05]],\n",
       "\n",
       "         [[ 4.8983e-01]],\n",
       "\n",
       "         [[ 5.0223e-01]],\n",
       "\n",
       "         [[ 4.8976e-01]],\n",
       "\n",
       "         [[ 7.4480e-06]],\n",
       "\n",
       "         [[ 4.9948e-01]],\n",
       "\n",
       "         [[ 5.0634e-01]],\n",
       "\n",
       "         [[ 4.9773e-01]],\n",
       "\n",
       "         [[ 5.0044e-01]],\n",
       "\n",
       "         [[-4.0606e-04]],\n",
       "\n",
       "         [[-6.5381e-05]],\n",
       "\n",
       "         [[ 4.9475e-01]],\n",
       "\n",
       "         [[ 5.0176e-01]],\n",
       "\n",
       "         [[ 5.0444e-01]],\n",
       "\n",
       "         [[ 5.0005e-01]],\n",
       "\n",
       "         [[ 5.0849e-01]],\n",
       "\n",
       "         [[ 5.0173e-01]],\n",
       "\n",
       "         [[ 4.9879e-01]],\n",
       "\n",
       "         [[ 4.9054e-01]],\n",
       "\n",
       "         [[ 4.9272e-01]],\n",
       "\n",
       "         [[ 4.9598e-01]],\n",
       "\n",
       "         [[ 6.3307e-02]],\n",
       "\n",
       "         [[ 5.1285e-01]],\n",
       "\n",
       "         [[ 1.4991e-01]],\n",
       "\n",
       "         [[ 5.1138e-01]],\n",
       "\n",
       "         [[ 5.0046e-01]],\n",
       "\n",
       "         [[ 4.9501e-01]],\n",
       "\n",
       "         [[ 5.1698e-01]],\n",
       "\n",
       "         [[ 4.5069e-01]],\n",
       "\n",
       "         [[ 5.1170e-01]],\n",
       "\n",
       "         [[ 5.0833e-01]],\n",
       "\n",
       "         [[ 4.1454e-06]],\n",
       "\n",
       "         [[ 4.9940e-01]],\n",
       "\n",
       "         [[ 5.3370e-01]],\n",
       "\n",
       "         [[ 3.7528e-05]],\n",
       "\n",
       "         [[ 4.8378e-01]],\n",
       "\n",
       "         [[ 3.8851e-02]],\n",
       "\n",
       "         [[ 1.3462e-04]],\n",
       "\n",
       "         [[ 4.9963e-01]],\n",
       "\n",
       "         [[ 5.1181e-01]],\n",
       "\n",
       "         [[ 5.0969e-01]],\n",
       "\n",
       "         [[ 4.1745e-01]],\n",
       "\n",
       "         [[-6.8314e-05]],\n",
       "\n",
       "         [[ 4.9978e-01]],\n",
       "\n",
       "         [[-1.9747e-03]],\n",
       "\n",
       "         [[ 4.9978e-01]],\n",
       "\n",
       "         [[ 5.0443e-01]],\n",
       "\n",
       "         [[ 5.0260e-01]],\n",
       "\n",
       "         [[-1.2562e-03]],\n",
       "\n",
       "         [[ 5.2120e-01]],\n",
       "\n",
       "         [[ 5.0497e-01]],\n",
       "\n",
       "         [[ 5.0404e-01]]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state['layer1.0.mask.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total1 = 0\n",
    "total2 = 0\n",
    "total3 = 0\n",
    "for idx, m in model.named_modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if idx.split('.')[0] == 'layer1':\n",
    "            total1 += m.weight.data.shape[0]\n",
    "        if idx.split('.')[0] == 'layer2':\n",
    "            total2 += m.weight.data.shape[0]\n",
    "        if idx.split('.')[0] == 'layer3':\n",
    "            total3 += m.weight.data.shape[0]\n",
    "\n",
    "bn1 = torch.zeros(total1)\n",
    "bn2 = torch.zeros(total2)\n",
    "bn3 = torch.zeros(total3)\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "index3 = 0\n",
    "for (idx, m) in model.named_modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        if idx.split('.')[0] == 'layer1':\n",
    "            size = m.weight.data.shape[0]\n",
    "            bn1[index1:(index1+size)] = m.weight.data.abs().clone()\n",
    "            index1 += size\n",
    "        if idx.split('.')[0] == 'layer2':\n",
    "            size = m.weight.data.shape[0]\n",
    "            bn2[index2:(index2+size)] = m.weight.data.abs().clone()\n",
    "            index2 += size\n",
    "        if idx.split('.')[0] == 'layer3':\n",
    "            size = m.weight.data.shape[0]\n",
    "            bn3[index3:(index3+size)] = m.weight.data.abs().clone()\n",
    "            index3 += size\n",
    "            \n",
    "y1, i = torch.sort(bn1)\n",
    "thre_index1 = int(total1 * 0.5)\n",
    "thre1 = y1[thre_index1]\n",
    "\n",
    "y2, i = torch.sort(bn2)\n",
    "thre_index2 = int(total2 * 0.5)\n",
    "thre2 = y2[thre_index2]\n",
    "\n",
    "y3, i = torch.sort(bn3)\n",
    "thre_index3 = int(total3 * 0.5)\n",
    "thre3 = y3[thre_index3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask): scaler()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules = list(model.named_modules())\n",
    "# for k, (idx, m) in enumerate(modules):\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = resnet50()\n",
    "\n",
    "\n",
    "pruned = 0\n",
    "cfg = []\n",
    "cfg_mask = []\n",
    "modules = list(model.named_modules())\n",
    "\n",
    "old_modules = list(model.modules())\n",
    "new_modules = list(newmodel.modules())\n",
    "cfg_dict = {}\n",
    "\n",
    "for k, (idx, m) in enumerate(modules):\n",
    "    \n",
    "    \n",
    "    \n",
    "    if isinstance(m, nn.BatchNorm2d) :\n",
    "        if idx.split('.')[0] == 'layer1':\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(thre1).float()\n",
    "#             m.wmul_eight.data = weight_copy*mask\n",
    "            \n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "#             if not isinstance(modules[k-2],nn.Sequential):\n",
    "#             print(idx.split('.')[1])\n",
    "            if not idx.split('.')[2]=='downsample':\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "            else:\n",
    "                print('yes')\n",
    "                print(torch.sum(mask),'**')\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            cfg_dict[idx] = mask\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif idx.split('.')[0] == 'layer2':\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(thre2).float()\n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "#             if not isinstance(modules[k-2],nn.Sequential):\n",
    "            if not idx.split('.')[2]=='downsample':\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "            else:\n",
    "                print('yes')\n",
    "                print(torch.sum(mask),'**')\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            cfg_dict[idx] = mask\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif idx.split('.')[0] == 'layer3':\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(thre3).float()\n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "            if not idx.split('.')[2]=='downsample':\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "            else:\n",
    "                print('yes')\n",
    "                print(torch.sum(mask),'**')\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            cfg_dict[idx] = mask\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "            \n",
    "        else:\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(0.0).float()\n",
    "#             pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "            try:\n",
    "                if not idx.split('.')[2]=='downsample':\n",
    "                    cfg.append(int(torch.sum(mask)))\n",
    "                    print(torch.sum(mask))\n",
    "                else:\n",
    "                    print('yes')\n",
    "                    print(torch.sum(mask),'**')\n",
    "            except:\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            cfg_dict[idx] = mask\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "    elif isinstance(m, nn.MaxPool2d):\n",
    "        cfg.append('M')\n",
    "\n",
    "pruned_ratio = pruned/(total1+total2+total3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/workspace/tracking_datasets/cfg_dict_resnet_child/cfg_dict_resnet50_layerwise_budget_50.json', 'wb') as fp:\n",
    "    pickle.dump(cfg_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/workspace/tracking_datasets/cfg_dict_resnet_child/cfg_dict_resnet50_layerwise_budget_50.json', 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = convert_resnet(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.state_dict()['layer1.0.f_delta.1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.state_dict()['layer1.0.f_delta.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(out.state_dict(),'/workspace/tracking_datasets/pruned_ckpts/dimp50_bar/correct_layerwise_pruned_50p.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.load_state_dict(torch.load('/workspace/tracking_datasets/pruned_ckpts/dimp50_bar/layerwise_pruned_50p.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load('/workspace/tracking_datasets/pruned_ckpts/dimp50_bar/layerwise_pruned_50p.pth')\n",
    "# ckpt['layer1.0.f_delta.1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/workspace/tracking_datasets/pruned_ckpts/dimp50_bar/correct_layerwise_pruned_50p.pth.tar')\n",
    "ckpt['layer1.0.f_delta.1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.rand((2,3,224,224))\n",
    "output = out(input1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "old_modules = list(model.modules())\n",
    "new_modules = list(newmodel.modules())\n",
    "layer_id_in_cfg = 0\n",
    "start_mask = torch.ones(3)\n",
    "end_mask = cfg_mask[layer_id_in_cfg]\n",
    "conv_count = 0\n",
    "first_bn = False\n",
    "\n",
    "for layer_id in range(len(old_modules)):\n",
    "    m = old_modules[layer_id]\n",
    "    m1 = new_modules[layer_id]\n",
    "    \n",
    "     if isinstance(m, nn.BatchNorm2d) :\n",
    "        if idx.split('.')[0] == 'layer1':\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(thre1).float()\n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "#             if not isinstance(modules[k-2],nn.Sequential):\n",
    "#             print(idx.split('.')[1])\n",
    "            if not idx.split('.')[2]=='downsample':\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "            else:\n",
    "                print('yes')\n",
    "                print(torch.sum(mask),'**')\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif idx.split('.')[0] == 'layer2':\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(thre2).float()\n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "#             if not isinstance(modules[k-2],nn.Sequential):\n",
    "            if not idx.split('.')[2]=='downsample':\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "            else:\n",
    "                print('yes')\n",
    "                print(torch.sum(mask),'**')\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "        elif idx.split('.')[0] == 'layer3':\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(thre3).float()\n",
    "            pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "            if not idx.split('.')[2]=='downsample':\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "            else:\n",
    "                print('yes')\n",
    "                print(torch.sum(mask),'**')\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "            \n",
    "        else:\n",
    "            weight_copy = m.weight.data.abs().clone()\n",
    "            mask = weight_copy.gt(0.0).float()\n",
    "#             pruned = pruned + mask.shape[0] - torch.sum(mask)\n",
    "            m.weight.data.mul_(mask)\n",
    "            m.bias.data.mul_(mask)\n",
    "            try:\n",
    "                if not idx.split('.')[2]=='downsample':\n",
    "                    cfg.append(int(torch.sum(mask)))\n",
    "                    print(torch.sum(mask))\n",
    "                else:\n",
    "                    print('yes')\n",
    "                    print(torch.sum(mask),'**')\n",
    "            except:\n",
    "                cfg.append(int(torch.sum(mask)))\n",
    "                print(torch.sum(mask))\n",
    "    #             print(modules[k-2])\n",
    "            cfg_mask.append(mask.clone())\n",
    "            print('layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}'.format(k, mask.shape[0], int(torch.sum(mask))))\n",
    "    elif isinstance(m, nn.MaxPool2d):\n",
    "        cfg.append('M')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/pytracking\n",
    "from ltr.admin.loading import torch_load_legacy\n",
    "ckpt1 = torch_load_legacy('/workspace/pytracking/pytracking/networks/dimp50.pth')['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt1['bb_regressor.conv3_1r.0.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/workspace/tracking_datasets/cfg_dict_resnet_child/cfg_dict_resnet50_layerwise_budget_50.json', 'rb') as fp:\n",
    "    cfg_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict['layer2.3.bn3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.logical_or(cfg_dict['layer2.0.downsample.1'],cfg_dict['layer2.0.bn3'])\n",
    "out = np.logical_or(out,cfg_dict['layer2.1.bn3'])\n",
    "out = np.logical_or(out,cfg_dict['layer2.2.bn3'])\n",
    "out = np.logical_or(out,cfg_dict['layer2.3.bn3'])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = np.logical_or(cfg_dict['layer3.0.downsample.1'],cfg_dict['layer3.0.bn3'])\n",
    "out1 = np.logical_or(out1,cfg_dict['layer3.1.bn3'])\n",
    "out1 = np.logical_or(out1,cfg_dict['layer3.2.bn3'])\n",
    "out1 = np.logical_or(out1,cfg_dict['layer3.3.bn3'])\n",
    "out1 = np.logical_or(out1,cfg_dict['layer3.4.bn3'])\n",
    "out1 = np.logical_or(out1,cfg_dict['layer3.5.bn3'])\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = np.where(out>0)[0]\n",
    "idx2 = np.where(out1>0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt1['bb_regressor.conv3_1r.0.weight'] = ckpt1['bb_regressor.conv3_1r.0.weight'][:,idx1,:,:]\n",
    "ckpt1['bb_regressor.conv3_1t.0.weight'] = ckpt1['bb_regressor.conv3_1t.0.weight'][:,idx1,:,:]\n",
    "\n",
    "ckpt1['bb_regressor.conv4_1r.0.weight'] = ckpt1['bb_regressor.conv4_1r.0.weight'][:,idx2,:,:]\n",
    "ckpt1['bb_regressor.conv4_1t.0.weight'] = ckpt1['bb_regressor.conv4_1t.0.weight'][:,idx2,:,:]\n",
    "\n",
    "ckpt1['classifier.feature_extractor.0.weight'] = ckpt1['classifier.feature_extractor.0.weight'][:,idx2,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt1['classifier.feature_extractor.0.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt1['bb_regressor.conv4_1t.0.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ckpt1,'/workspace/pytracking/pytracking/networks/dimp50_bar.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt1 = ckpt1['bb_regressor.conv3_1r.0.weight'][:,idx1,:,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
